{"title":"数理統計学","markdown":{"yaml":{"title":"数理統計学","number-sections":true},"headingText":"分布概要","containsRefs":false,"markdown":"\n\n\n$$\n% physicsを使えるようにする\n\\require{physics}\n$$\n\n### --- 離散型分布 --- {.unnumbered}\n\n### 2項分布\n\n::: outline\n\n- 表記: $Bin(n, p)$\n- パラメータ\n  - 試行回数: $n$\n  - 成功確率: $p$\n- 確率関数:\n$$p (k) = \\binom{n}{k} p^k (1-p)^{n-k}$$\n- 確率母関数:\n$$G (s) = (1 + p (s - 1))^n$$\n- 期待値・分散:\n$$E \\qty [ X] = np, \\quad Var \\qty [ X ] = np (1 - p)$$\n- 再生性:\n    \n    $X_1 \\sim Bin(n_1, p), X_2 \\sim Bin(n_2, p)$ のとき、\n    $Y = X_1 + X_2$は$Bin(n_1 + n_2, p)$に従う。\n:::\n\n### ポアソン分布\n\n::: outline\n- 表記:\n$Po(\\lambda)$\n\n- パラメータ\n    - 平均: $\\lambda$\n\n- 確率関数:\n$$p (k) = \\frac{\\lambda^{k}}{k!} e^{-\\lambda}$$\n\n- 期待値・分散:\n$$E \\qty [ X] = \\lambda , \\quad Var \\qty [ X ] = \\lambda$$\n:::\n\n### 幾何分布\n\n::: outline\n- 意味:\n\n    ベルヌーイ試行の成功確率を $p$ とし、\n    確率変数 $X$ を1回成功するまでの失敗の回数とする。\n    $X$ はパラメータ $p$ の幾何分布に従う。\n\n- 表記:\n$Ge(p)$\n\n- パラメータ\n    - 成功確率: $p$\n\n- 確率関数:\n$$p (k) =  p(1-p)^{k}$$\n\n- 確率母関数:\n$$G (s) = \\frac{p}{1 - qs} = \\qty ( 1 - (s - 1) \\frac{q}{p})^{-1}$$\n\n- 期待値・分散:\n$$E \\qty [ X] = \\frac{1 - p}{p}, \\quad Var \\qty [ X ] = \\frac{1 - p}{p^2}$$\n:::\n\n#### 無記憶性\n\n幾何分布は無記憶性を持つ\n$$\n    P(X = x + x_0 | X \\geq x_0) = P(X = x)\n$${#eq-geometric_distribution_memorylessness}\n\n:::{.proof}\n## 無記憶性を持つことの証明\n\nまず、$P(X \\geq x_0)$ を計算する。無限級数の公式より\n$$\nP(X \\geq x_0) = \\sum_{x = x_0}^{\\infty} p(1-p)^{x}\n= \\frac{p(1 - p)^{x_0}}{1 - (1 - p)} = (1 - p)^{x_0}\n$$ となる。\n次に @eq-geometric_distribution_memorylessness を確かめる。 [^1]\n\n[^1]: 1行目の等号は丁寧に書くと \n$$\nP(X = x + x_0 | X \\geq x_0) \n    = \\frac{P(X = x + x_0, X \\geq x_0)}{P(X \\geq x_0)}\n$$\nであるが、 $A = \\{\\xi | \\xi = x + x_0\\}, B = \\{\\xi | x \\geq x_0\\}$ とすると\n$A \\subset B$であるので、$A \\cap B = A$ つまり\n$P(X = x + x_0, X \\geq x_0) = P(X = x + x_0)$ となり\n@eq-geometric_distribution_memorylessness_proof のようになる\n\n$$\n\\begin{aligned}\n    P(X = x + x_0 | X \\geq x_0) &= \\frac{P(X = x + x_0)}{P(X \\geq x_0)}  \\\\\n    &= \\frac{p(1-p)^{x + x_0}}{(1-p)^{x_0}} \\\\\n    &= p(1-p)^{x} = P(X = x)\n\\end{aligned}\n$${#eq-geometric_distribution_memorylessness_proof}\n\n以上から幾何分布が無記憶性を持つことがわかった。\n:::\n\n:::{.proof}\n## 非負整数上の確率分布が無記憶性を持つことの必要十分条件はその確率分布が幾何分布であることの証明\n\n十分性は上で示したので、必要性を示す。\n@eq-geometric_distribution_memorylessness が成立するとき、\n$$\nP(X = x + x_0) = P(X \\geq x_0) P(X = x)\n$$\nとなり、とくに $x_0 = 1$ のとき\n$$\n\\begin{aligned}\nP(X = x + 1) &= P(X \\geq 1) P(X = x) \\\\\n&= (1 - P(X = 0)) P(X = 0) \\\\ \n&= (1 - p) P(X = 0)\n\\end{aligned}\n$$\nという漸化式が得られる。ここで $P(X=0) = p$ とおいた。 この漸化式より\n$$\nP(X = x) = (1 - p) P(X = x - 1) = (1 - p)^{x} P(X = 0) = (1 - p)^{x}p\n$$\nとなり、幾何分布の確率関数になることがわかる。これで必要性が示された。\n:::\n\n### 負の2項分布\n\n::: outline\n- 意味:\n\n    ベルヌーイ試行の成功確率を $p$ とし、\n    確率変数 $X$ を $r$ 回成功するまでの失敗の回数とする。\n    $X$ は負の2項分布に従う。\n\n- 表記:\n$NB(r, p)$\n\n- パラメータ\n    - 成功回数: $r$\n    - 成功確率: $p$\n\n- 確率関数:\n$$\n\\begin{aligned}\np (k) &= \\binom{r + k - 1}{k} p^r (1-p)^{k} \\\\\n&= {}_{r} H_{k} ~ p^r (1-p)^{k}\n\\end{aligned}\n$$\n\n- 確率母関数:\n$$\n\\begin{aligned}\nG (s) &= \\qty (\\frac{p}{1 - qs})^{r} \\\\ \n&= \\qty ( 1 - (s - 1) \\frac{q}{p})^{-r}\n\\end{aligned}\n$$\n\n- 期待値・分散:\n$$\nE \\qty [ X] = \\frac{r(1 - p)}{p}, \\quad\nVar \\qty [ X ] = \\frac{r(1 - p)}{p^2}\n$$\n\n- 再生性:\n\n    $X_1 \\sim NB(r_1, p), X_2 \\sim NB(r_2, p)$ のとき、\n    $Y = X_1 + X_2$は$NB(r_1 + r_2, p)$に従う。\n:::\n\n#### 確率関数の意味\n\n確率関数に重複組合せ ${}_{r} H_{k}$ が出てくる意味について説明する。\\\n$r$ 回の成功と $x$ 回の失敗の並び方の場合の数を考える。\n最後は成功となることが決まっているので、それぞれの失敗は\nはじめ～1回目の成功の間、1回目の成功～2回目の成功の間、・・・、 $r-1$ 回目の成功～ $r$ 回目の成功の間\nのどこかに位置することになる。この並び方の場合の数は重複組合せ ${}_{r} H_{k}$ となる。\n\n```{mermaid}\n%%| fig-cap: \"A1からArのどこに失敗が位置するかの場合の数は重複組み合わせになる\"\n%%| label: fig-negative-binomial\ngraph LR\nA[はじめ] -- A1 --> B[1回目の成功]\nB -- A2 --> C[2回目の成功]\nC -- A3 --> D[\"...\"]\nD -- Ar-1 --> E[r-1回目の成功]\nE -- Ar --> F[r回目の成功]\n```\n\n### 超幾何分布\n\n::: outline\n- 表記:\n$HG(N, M, n)$\n\n- パラメータ\n    - くじの総数: $N$\n    - あたりの数: $M$\n    - くじを引く回数: $n$\n\n- 確率関数:\n$$\np (x) = \\frac{{}_M C_{x} \\times {}_{N-M} C_{n-x}}{{}_{N} C_{n}} , \\quad \n    \\max \\{0, n - (N - M)\\} \\leq x \\leq \\min \\{n, M\\}\n$$\n\n- 確率母関数:\n\n    初等的な関数では表せない\n\n- 期待値・分散:\n$$\nE \\qty [X] = n \\frac{M}{N}, \\quad \n    Var \\qty [ X ] = n \\frac{M}{N} \\qty (1 - \\frac{M}{N}) \\times \\frac{N - n}{N - 1}\n$$\n:::\n\n#### 確率関数の導出\n\n$X = x$ となる確率を求める。\n$N$ 個のくじから $n$ 個を引く場合の数は ${}_{N} C_{n}$ である。\nこのうち、あたりを $x$ 個引く場合の数は、\n$M$ 個のあたりから $x$ 個をひき、 $N - M$ 個のはずれから $n - x$ 個ひく場合の数を考えれば良いので\n${}_{M} C_{x} \\times {}_{N - M} C_{n - x}$ となる。 よって、求める確率は\n$$\nP(X = x) = p(x) = \\frac{{}_M C_{x} \\times {}_{N-M} C_{n-x}}{{}_{N} C_{n}}\n$$\nとなる。$x$ の範囲は以下のように決められる。\n${}_M C_x = {}_M C_{M-x}$ であり、 $x \\geq 0, M-x \\geq$ を満たす必要がある。\nつまり $$x \\geq 0, \\quad M \\geq x$$ という条件が得られる。\n同様に、${}_{N - M} C_{n - x} = {}_{N - M} C_{N - M - n + x}$ であり、\n$n - x \\geq 0, N - M - n + x \\geq 0$ を満たす必要がある。 つまり\n$$\nx \\geq n - (N - M), \\quad m \\geq x\n$$\nという条件が得られる。\n以上の条件を合わせて\n$$\n\\max \\{0, n - (N - M)\\} \\leq x \\leq \\min \\{n, M\\}\n$$ となる。\n\n### 多項分布\n\n::: outline\n- 確率関数\n$$\\begin{aligned}\n    p(x_1, x_2, \\dots, x_k) & = \\frac{n!}{x_1! x_2! \\dots x_k!} p_1^{x_1} p_2^{x_2} \\dots p_k^{x_k}   \\\\\n                            & \\qquad (p_1 + p_2 + \\dots + p_k = 1, \\quad x_1 + x_2 + \\dots + x_k = n)\n    \\end{aligned}$$\n:::\n\n### --- 連続型分布 --- {.unnumbered}\n\n### 正規分布\n\n::: outline\n- 表記:\n$N(\\mu, \\sigma^2)$\n\n- パラメータ\n    - 平均: $\\mu$\n    - 分散: $\\sigma^2$\n\n- 密度関数:\n$$\nf (x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp \\qty ( - \\frac{(x - \\mu)^2}{2 \\sigma^2} )\n$$\n\n- 積率母関数:\n$$\n\\phi(\\theta) = \\exp \\qty ( \\mu \\theta + \\frac{\\sigma^2}{2} \\theta^2 )\n$$\n\n- 期待値・分散:\n$$\nE \\qty [X] = \\mu, \\quad Var \\qty [ X ] = \\sigma^2\n$$\n\n- 再生性:\n    \n    $X_1 \\sim N(\\mu_1, \\sigma_1^2), X_2 \\sim N(\\mu_2, \\sigma_2^2)$ のとき、\n    $Y = X_1 + X_2$ は $N(\\mu_1 + \\mu_2, \\sigma_1^2 + \\sigma_2^2)$ に従う。\n:::\n\n#### 積率母関数の計算\n\n$$\n\\begin{aligned}\n    \\phi(\\theta) & = E \\qty [e^{\\theta X}]                                                                                                                                                                                           \\\\\n    & = \\int_{- \\infty}^{\\infty} \\dd{x} e^{\\theta x} \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\qty ( - \\frac{(x - \\mu)^2}{2 \\sigma^2} )                                                                                     \\\\\n    & = \\int_{- \\infty}^{\\infty} \\dd{x}  \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\qty ( - \\frac{(x - \\mu)^2}{2 \\sigma^2} + \\theta x )                                                                                      \\\\\n    & = \\int_{- \\infty}^{\\infty} \\dd{x}  \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\qty ( -\\frac{1}{2 \\sigma^2} \\qty (x - (\\mu + \\sigma^2 \\theta))^2 - \\frac{-(\\mu + \\sigma^2 \\theta)^2 + \\mu^2}{2 \\sigma^2} ) & (平方完成)      \\\\ \n    & = \\int_{- \\infty}^{\\infty} \\dd{x}  \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\qty ( -\\frac{1}{2 \\sigma^2} \\qty (x - (\\mu + \\sigma^2 \\theta))^2) \\exp \\qty ( \\mu \\theta + \\frac{\\sigma^2}{2} \\theta^2 )                 \\\\\n    & = \\exp \\qty ( \\mu \\theta + \\frac{\\sigma^2}{2} \\theta^2 )                                                                                                                                            & (ガウス積分を実行)\n\\end{aligned}\n$$\n\n#### 標本平均と標本分散が独立であること\n\n以下のことを示す\n\n::: outline\n- $\\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_i$ と\n$s^2 = \\frac{1}{n - 1} \\sum_{i=1}^{n} (X_i - \\bar{X})^2$ が互いに独立である\n- $\\frac{(n - 1) s^2}{\\sigma^2}$ は自由度 $n-1$ のカイ2乗分布に従う\n:::\n\n標準正規分布でない場合は標準化したものを考えればよいので、\n一般性を失わず、\n$X_1, \\dots, X_n \\sim N(0, 1), i.i.d.$ とする。\nベクトルの形で $\\va{X} = (X_1, \\dots, X_n)^{T}$ とすると、\n$\\va{X} \\sim N_n(\\va{0}, I_n)$ である。\n\n直交行列 $G$ を考える。$G$ の一行目はすべて $\\frac{1}{\\sqrt{n}}$ であるとする。\n他の行は互いに直行するように構成すれば良い。\n$\\va{Y}$ を $\\va{Y} = G\\va{X}$ で定義する。\n$G$ は直交行列なので、$\\va{Y} \\sim N_n(\\va{0}, I_n)$ となり、\n$Y_1, \\dots, Y_n$ は互いに独立となる。 このとき、\n$$\n\\sum_{i=1}^{n} Y_i^2 = \\va{Y}^{T} \\va{Y}\n  = \\va{X}^T G^{T} G \\va{X} = \\va{X}^{T} \\va{X} = \\sum_{i=1}^{n} X_i^2\n$$\n\nまた、 $$Y_1 = \\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} X_i = \\sqrt{n}\\bar{X}$$\nとなる。 \n偏差平方和を計算すると \n$$\n\\sum_{i=1}^{n} (X_i - \\bar{X})^2 \n  = \\sum_{i=1}^{n} X_i^2 - 2 \\bar{X} \\sum_{i=1}^{n} X_i + n \\bar{X}^2\n  = \\sum_{i=1}^{n} Y_i^2 - n \\bar{X}^2\n  = \\sum_{i=1}^{n} Y_i^2 - Y_1^2\n  = \\sum_{i=2}^{n} Y_i^2\n$$\nとなる。\nこれは互いに独立な標準正規分布に従う変数の二乗和であるから、\n$\\sum_{i=2}^{n} Y_i^2 = \\frac{(n - 1) s^2}{\\sigma^2}$ が自由度 $n-1$ のカイ2乗分布に従うことがわかった。\nまた、 $s^2 = \\frac{1}{n - 1} \\sum_{i=1}^{n} (X_i - \\bar{X})^2$ が\n$Y_2, \\dots, Y_n$ のみに依存し、 $Y_1 = \\sqrt{n} \\bar{X}$ に依存しないことから\n$s^2$ と $\\bar{X}$ が独立であることがわかった。\n\n### 多変量正規分布\n\n::: outline\n- 表記:\n$N_n(\\va{\\mu}, \\Sigma)$\n\n- パラメータ:\n    - 平均ベクトル: $\\va{\\mu}$\n    - 分散共分散行列: $\\Sigma$\n\n- 密度関数:\n$$\nf (\\va{x}) \n= \\frac{1}{\\sqrt{2\\pi}^n \\sqrt{|\\Sigma|}} \n\\exp \\qty ( - \\frac{1}{2} (\\va{x} - \\va{\\mu})^{T} \\Sigma^{-1} (\\va{x} - \\va{\\mu}) )\n$$\n\n- 積率母関数:\n$$\n\\phi(\\va{\\theta}) \n= \\exp \\qty ( \\va{\\mu}^{T} \\va{\\theta} + \\frac{1}{2} \\va{\\theta}^{T} \\Sigma \\va{\\theta} )\n$$\n\n- 期待値・分散:\n$$\nE \\qty [\\va{X}] = \\va{\\mu}, \\quad Var \\qty [ \\va{X} ] = \\Sigma\n$$\n\n- 再生性:\n\n    $\\va{X}_1 \\sim N(\\va{\\mu}_1, \\Sigma_1), \\va{X}_2 \\sim N(\\va{\\mu}_2, \\Sigma_2)$ のとき、\n    $\\va{Y} = \\va{X}_1 + \\va{X}_2$は$N(\\va{\\mu}_1 + \\va{\\mu}_2, \\Sigma_1 + \\Sigma_2)$ に従う。\n:::\n\n#### 精度行列\n\n#### 線形変換\n\n#### 周辺分布\n\n#### 条件付き分布\n\n### カイ2乗分布\n\n::: outline\n- 定義:\n\n    $Z_i \\sim N(0, 1), i = 1, \\dots, n$ で、これらが互いに独立なとき\n    $Y = Z_1^2 + \\dots + Z_n^2$ が従う分布を自由度 $n$ のカイ2乗分布という。\n\n- 表記:\n    $\\chi^2(n)$\n\n- パラメータ\n    - 自由度: $n$\n\n- 密度関数:\n$$\nf (x) = \\frac{1}{\\Gamma(\\frac{n}{2}) 2^{\\frac{n}{2}}}\n    y^{\\frac{n}{2} - 1} e^{- \\frac{y}{2}}, \\quad (y > 0)\n$$\n\n- 積率母関数:\n$$\\phi(\\theta) =$$\n\n- 期待値・分散:\n$$\nE \\qty [X] = n, \\quad Var \\qty [ X ] = 2n\n$$\n\n- 再生性:\n\n    $X_1 \\sim \\chi^2(n_1), X_2 \\sim \\chi^2(n_2)$ のとき、\n    $Y = X_1 + X_2$ は $\\chi^2(n_1 + n_2)$ に従う。\n:::\n\n### $F$ 分布\n\n::: outline\n- 定義:\n\n    $X_1 \\sim \\chi^2(n_1), X_2 \\sim \\chi^2(n_2)$\n    でこれらが互いに独立なとき、 \n    $$\n    \\frac{X_1/n_1}{X_2/n_2}\n    $$\n\n    が従う分布を自由度 $(n_1, n_2)$ の $F$ 分布という。 \n    \n- 表記:\n$F(n_1, n_2)$\n\n- パラメータ:\n    - 自由度: $(n_1, n_2)$\n\n- 密度関数:\n$$\nf (x) =\n$$\n\n- 積率母関数:\n$$\n\\phi(\\theta) =\n$$\n\n- 期待値・分散:\n$$\nE \\qty [X] = , \\quad Var \\qty [ X ] =\n$$\n:::\n\n### $t$ 分布\n\n::: outline\n$Z \\sim N(0, 1), Y \\sim \\chi^2(n)$ で、これらが互いに独立なとき、\n$$T = \\frac{Z}{\\sqrt{Y/n}}$$ が従う分布を自由度$n$の$t$分布という。\n\n$$t(n)$$\n\n$n$\n\n$$f (x) = \\frac{\\Gamma(\\frac{n+1}{2})}{\\sqrt{\\pi n}\\Gamma{\\frac{n}{2}}}\n    \\qty ( 1 + \\frac{t^2}{n} )^{- \\frac{n+1}{2}}$$\n\n$$\\phi(\\theta) =$$\n\n$$E \\qty [X] = 0 \\quad (n > 1), \\quad Var \\qty [ X ] = \\frac{n}{n - 2} \\quad (n > 2)$$\n:::\n\n### 指数分布\n\n::: outline\n$$Ex(\\lambda)$$\n\n$\\lambda > 0$\n\n$$f(x) = \\lambda e^{- \\lambda x}, \\quad x > 0$$\n\n$$\\phi (\\theta) = \\frac{\\lambda}{\\lambda - \\theta}$$\n\n$$E \\qty [ X] = \\frac{1}{\\lambda}, \\quad Var \\qty [ X ] = \\frac{1}{\\lambda^2}$$\n:::\n\n#### 無記憶性\n\n指数分布は無記憶性を持つ\n$$\\label{eq:exponential_distribution_memorylessness}\n  P(T = t + t_0 | T \\geq t_0) = P(T = t)$$\n\n##### 無記憶性を持つことの証明\n\n$T \\sim Ex(\\lambda)$とする。\n$T$の確率密度関数は$f(t) = \\lambda e^{-\\lambda t}$である。\n$T > t_0$が与えられたときの$T$の条件付き確率密度関数を求める。\n$$\\begin{aligned}\n    f_{T | T > t_0} (t) \\dd{t} &= P( t \\leq T \\leq t + \\dd{t} | T > t_0) \\\\\n    &= \\frac{P( t \\leq T \\leq t + \\dd{t} , T > t_0)}{P(T > t_0)}  \\\\\n    &= \\frac{P( t \\leq T \\leq t + \\dd{t} , t > t_0)}{\\int_{t_0}^{\\infty}\\dd{t}f(t)} \\\\\n    &= \\frac{f(t)\\dd{t} \\times I_{[t>t_0]}(t)}{e^{-\\lambda t_0}} \\\\\n    &= \\lambda e^{-\\lambda (t - t_0)} I_{[t>t_0]}(t) \\dd{t}\n  \\end{aligned}$$ であるから、\n$$f_{T | T > t_0} (t) =  \\lambda e^{-\\lambda (t - t_0)}  \\quad (t > t_0)$$\nとなる。\nこれは基準点を$t_0$にずらした変数$\\tilde{T} = T - t_0$の確率密度関数と等しくなる。\n指数分布が無記憶性を持つことがわかった。\n\n##### 正の実数上の確率分布が無記憶性を持つことの必要十分条件はその確率分布が指数分布であることの証明\n\n十分性は上で示したので、必要性を示す。\n[\\[eq:exponential_distribution_memorylessness\\]](#eq:exponential_distribution_memorylessness){reference-type=\"ref\"\nreference=\"eq:exponential_distribution_memorylessness\"}が成り立つとき、\n$$f(t + t_0) = (1 - F(t_0)) f(t) = f(t) - F(t_0) f(t)$$\nとなる。ここで$t_0 = \\Delta t$と書き、$\\Delta t$が小さい時を考える。\nすると $$\\frac{f(t + \\Delta t) - f(t)}{\\Delta t} =\n   - \\frac{1}{\\Delta t} \\qty ( \\int_{0}^{\\Delta t} \\dd{t} f(t)) f(t)$$\nと書くことができ、$\\Delta t \\rightarrow 0$で上式は\n$$f'(t) = - f(0) f(t)$$\nという微分方程式が得られる。$f(0)$は定数であるので、$f(0)=\\lambda$とおく。\nこの微分方程式を解くと $$f(t) = \\lambda e^{- \\lambda t}$$\nとなる。ここで全体にかかる係数$\\lambda$は上で決めた$f(0)=\\lambda$から定まる。\nこれは指数分布の密度関数である。これで必要性が示された。\n\n### ガンマ分布\n\n![ガンマ分布の密度関数](figures/gamma_pdf.png){width=\"15cm\"}\n\n::: outline\n$$Ga(\\nu, \\alpha)$$\n\n$\\nu > 0$ $\\alpha > 0$\n\n$$f(x) = \\frac{1}{\\alpha^{\\nu} \\Gamma (\\nu)} x ^ {\\nu - 1} e ^ {- \\frac{x}{\\alpha}}, \\quad x >0$$\n\n$$\\phi (\\theta) = ( 1 - \\theta \\alpha) ^ {- \\nu}$$\n\n$$E \\qty [ X] = \\nu \\alpha, \\quad Var \\qty [ X ] = \\nu \\alpha ^2$$\n\n$X_1 \\sim Ga(\\nu_1, \\alpha), X_2 \\sim Ga(\\nu_2, \\alpha)$ のとき、\n$Y = X_1 + X_2$は$Ga(\\nu_1 + \\nu_2, \\alpha)$に従う。\n:::\n\n#### 期待値・分散の計算\n\n$X \\sim Ga(\\nu, 1)$とする。\n\n$X$の期待値 $$\\begin{aligned}\n    E \\qty [X] & = \\int_0^{\\infty} \\frac{x}{\\Gamma(\\nu)} x^{\\nu - 1} e^{-x} \\dd{x}\n    = \\frac{1}{\\Gamma(\\nu)} \\int_0^{\\infty}  x^{(\\nu + 1) - 1} e^{-x} \\dd{x}       \\\\\n               & = \\frac{\\Gamma(\\nu + 1)}{\\Gamma(\\nu)}\n    = \\frac{\\nu !}{(\\nu - 1)!}                                                     \\\\\n               & = \\nu\n  \\end{aligned}$$\n\n$X^2$の期待値 $$\\begin{aligned}\n    E \\qty [X^2] & = \\int_0^{\\infty} \\frac{x^2}{\\Gamma(\\nu)} x^{\\nu - 1} e^{-x} \\dd{x}\n    = \\frac{1}{\\Gamma(\\nu)} \\int_0^{\\infty}  x^{(\\nu + 2) - 1} e^{-x} \\dd{x}           \\\\\n                 & = \\frac{\\Gamma(\\nu + 2)}{\\Gamma(\\nu)}\n    = \\frac{(\\nu + 1)!}{(\\nu - 1)!}                                                    \\\\\n                 & = (\\nu + 1)\\nu\n  \\end{aligned}$$\n\n$X$の分散\n$$Var \\qty [X] = E \\qty [X^2] - (E \\qty [X])^2 = (\\nu + 1)\\nu - \\nu^2 = \\nu$$\n\n$Y = \\alpha X$とすると、$Y \\sim Ga(\\nu, \\alpha)$となる。\n\n$Y$の期待値 $$E \\qty [Y] = E \\qty [\\alpha X] = \\alpha \\nu$$\n\n$Y$の分散 $$Var \\qty [Y] = Var \\qty [\\alpha X] = \\alpha^2 \\nu$$\n\n### ベータ分布\n\n$X \\sim Be(\\alpha, \\beta)$のとき、$1 - X \\sim Be(\\beta, \\alpha)$\n\n## 分布の間の関係\n\n### ガンマ分布とカイ2乗分布\n\n$Ga(\\frac{n}{2}, 2)$は自由度$n$のカイ2乗分布になる\n\n### ガンマ分布と指数分布\n\n$Ga(1, \\frac{1}{\\lambda})$はパラメータ$\\lambda$の指数分布$Ex(\\lambda)$になる\n\n### ガンマ分布と負の二項分布\n\nガンマ分布は負の2項分布を連続変数化したものと考えられる\n\n::: outline\nポアソン分布に従うイベントが$\\nu$回起こるまでの時間。\nポアソン分布に従うイベントが1回起こるまでの時間。\n\nベルヌーイ試行が$r$回成功するまでの失敗の回数。\nベルヌーイ試行が1回成功するまでの失敗の回数。\n:::\n\n### 二項分布とポアソン分布\n\n$$\\begin{aligned}\n    p(x) &= \\binom{n}{x} p^{x} (1 - p)^{n -x} \\\\\n    &= \\frac{n!}{x! (n - x)!} \\qty (\\frac{\\lambda}{n})^{x} \\qty (1 - \\frac{\\lambda}{n})^{n - x}  & (\\lambda = np) \\\\\n    &= \\frac{1}{x!} n \\cdot (n - 1) \\cdot \\dots \\cdot (n - x + 1) \\qty (\\frac{\\lambda}{n})^{x} \\qty (1 - \\frac{\\lambda}{n})^{n - x} \\\\\n    &= \\frac{1}{x!} 1 \\cdot \\qty(1 - \\frac{1}{n}) \\cdot \\dots \\cdot \\qty (1 - \\frac{x - 1}{n})\n    \\lambda^{x} \\qty (1 - \\frac{\\lambda}{n})^{n}  \\qty (1 - \\frac{\\lambda}{n})^{-x} \\\\\n    &= \\frac{1}{x!} 1 \\cdot \\qty(1 - \\frac{1}{n}) \\cdot \\dots \\cdot \\qty (1 - \\frac{x - 1}{n})\n    \\lambda^{x} \\qty { \\qty (1 - \\frac{\\lambda}{n})^{-\\frac{n}{\\lambda}} }^{- \\lambda} \\qty (1 - \\frac{\\lambda}{n})^{-x} \\\\\n    &\\rightarrow \\frac{\\lambda^x}{x!}e^{-\\lambda} & (n \\rightarrow \\infty)\n  \\end{aligned}$$\n\n### 指数分布とポアソン分布\n\n#### ポアソン分布から指数分布を導く\n\n単位時間あたり平均で$\\lambda$回ランダムに発生するイベントが\n時間$(0, t)$の間に発生する回数は$Po(\\lambda t)$に従う。\n$X \\sim Po(\\lambda t)$とする。$X$の確率関数は\n$$p(x) = \\frac{(\\lambda t)^x}{x!}e^{-\\lambda t}$$ である。\nイベントが1回発生するまでの時間を表す確率変数を$T$とする。\n$T$の分布関数は $$\\begin{aligned}\n    F(t) &= P(T \\leq t) \\\\\n    &= 1 - P(T > t) \\\\\n    &= 1 - (時間(0, t)内に一回もイベントが発生しない確率) \\\\\n    &= 1 - P(X = 0) \\\\\n    &= 1 - p(x=0) \\\\\n    &= 1 - e^{-\\lambda t}\n  \\end{aligned}$$ となり、これは指数分布の分布関数である。\n密度関数を計算すると $$f(t) = \\dv{t} F(t) = \\lambda e^{- \\lambda t}$$\nとなる。\n\n#### 指数分布からポアソン分布を導く\n\n$T_i$をイベントが発生するまでの時間を表す確率変数とし、$T_i \\sim Ex(\\lambda)$とする。\n$$U_k = \\sum_{i=1}^{k} T_i$$ とし、$X$を\n$$X = (U_k < t を満たす最大のk)$$\nと定義する。つまり$X$は時刻$t$までに発生したイベントの回数を表す確率変数である。\n$X=k$となる確率を求めると $$\\begin{aligned}\n    P(X = k) &= (k回目のイベントが時刻(0, t)の間に発生し、k+1回目のイベントは時刻tより後に発生する確率) \\\\\n    &= \\int_{0}^{t} P(U_k = s, U_{k+1} > t) \\dd{s} \n    = \\int_{0}^{t} P(U_k = s) P(U_{k+1} > t | U_k = s) \\dd{s} \\\\\n    &= \\int_{0}^{t} P(U_k = s) P(T_{k+1} > t-s) \\dd{s} \n    = \\int_{0}^{t} \\frac{\\lambda^k}{\\Gamma(k)} s^{k-1} e^{-\\lambda t} \n    e^{- \\lambda (t-s)} \\dd{s}  \\\\\n    &= \\frac{\\lambda^k}{(k -1)!} e^{-\\lambda t} \\int_{0}^{t} s^{k-1} \\dd{s} \n    = \\frac{\\lambda^k}{(k -1)!}e^{-\\lambda t} \\frac{1}{k} t^{k} \\\\\n    &= \\frac{(\\lambda t)^k}{k!}e^{-\\lambda t}\n  \\end{aligned}$$ となり、これは$Po(\\lambda t)$の確率関数である。\n途中で$U_k \\sim Ga(k, 1/\\lambda)$であることを使った。\n\n### 指数分布と幾何分布\n\n$T$をイベントが発生するまでの時間を表す確率変数とし、$T \\sim Ex(\\lambda)$とする。\n時間を間隔$d$で離散化する。\n離散化した各時間の中で初めてイベントが発生する確率は $$\\begin{aligned}\n  &時間(0, d]の間に初めてイベントが発生する確率 &&= P(T \\leq d) = 1 - e^{- \\lambda d} \\\\\n  &時間(d, 2d]の間に初めてイベントが発生する確率 = P(T \\leq 2d | T > d) &&= P(T \\leq d) = 1 - e^{- \\lambda d} \\\\\n  &時間(2d, 3d]の間に初めてイベントが発生する確率 = P(T \\leq 3d | T > 2d) &&= P(T \\leq d) = 1 - e^{- \\lambda d} \\\\\n  & &&\\vdots\n\\end{aligned}$$ となる。ここで指数分布の無記憶性を使った。\nつまり各時間間隔$d$のなかで初めてイベントが発生する確率は常に等しく\n$p = 1 - e^{-\\lambda d}$となる。\n\n$x$を整数とする。時刻$xd$までイベントが発生せず、\n時間$(xd, (x+1)d]$の間に初めてイベントが起こったとき$X=x$となるように確率変数$X$を定める。\nこのとき、$X$はパラメータ$p = 1 - e^{-\\lambda d}$の幾何分布$Ge(p)$に従う。\n\n## 十分統計量\n\n### 概要\n\n#### 十分統計量の定義\n\n$k$個の統計量$T = ( T_1, \\dots, T_k )$がパラメータ$\\theta$に関する$k$次元の十分統計量であるとは、\n$T$を与えられたとき$X = ( X_1, \\dots, X_n )$の条件付き分布が$\\theta$に依存しないことである。\n$$\\label{eq:sufficient_statistic_definition}\n  P(X | T, \\theta) = P(X | T)$$\n\nベイズ統計のように$\\theta$を確率変数として考えれば、[\\[eq:sufficient_statistic_definition\\]](#eq:sufficient_statistic_definition){reference-type=\"ref\"\nreference=\"eq:sufficient_statistic_definition\"}は、\n$T$を与えたときに$X$と$\\theta$が条件付き独立となることを意味する。\n$X$と$\\theta$が条件付き独立ということは、$T$さえ知っていればそれ以上$X$から$\\theta$の情報が得られないということ。\nつまり$T$だけ知っていれば$X$から得られる$\\theta$に関する情報はすべてわかるということ。\n\n#### 分解定理\n\n$X$を離散確率変数または連続確率変数とし$p_\\theta$を$X$の確率関数または密度関数とする。\n$T(X) = ( T_1(X), \\dots, T_k(X))$が十分統計量であるための必要十分条件は$p_\\theta$が\n$$p_\\theta (x) = g_\\theta (T(x)) h(x)$$\n\nの形に分解できることである。ここで$h(x)$は$\\theta$を含まない$x$のみの関数である。\n\n#### ラオ・ブラックウェルの定理\n\n$\\delta(X)$を未知パラメータ$\\theta$の推定量とし、\n平均二乗誤差$R(\\theta, \\delta) = E_{\\theta} \\qty [(\\delta(X) - \\theta)^2]$をリスク関数とする。\nここで、十分統計量$T$を与えたときの$X$の条件付き分布を用いて$\\delta(X)$の条件付き期待値をとったものを$\\delta^{*}(T)$とする。\nすなわち $$\\delta^{*}(T) = E \\qty [ \\delta(X) | T ]$$\nである。このとき次のことが成り立つ。\n$$E_{\\theta} \\qty [(\\delta^{*}(T) - \\theta)^2] \\leq E_{\\theta} \\qty [(\\delta(X) - \\theta)^2], \\quad \\forall \\theta$$\nであり、等号が成立するのは$P_{\\theta}(\\delta(X) = \\delta^{*}(T)) = 1$となるときのみである。\n[^2]\n\n#### 完備性の定義\n\n統計量$T(X)$が完備であるとは、$T$の関数$g(T)$のなかで恒等的にその期待値が0となるものは、\n定数0に限るということである。 すなわち、任意の関数$g(T)$に対し\n$$E_{\\theta} \\qty [g(T)] = 0, \\quad \\forall \\theta \\Rightarrow g(T) \\equiv 0$$\nが成り立つならば$T$は完備である。 [^3]\n\n### ポアソン分布\n\n##### 十分統計量の計算\n\n$X_1, \\dots, X_n \\sim Po(\\lambda), i.i.d.$とする。確率関数は\n$$p_\\lambda(x) = \\prod_{i=1}^{n} \\frac{\\lambda^{x_i}}{x_i !} e^{- \\lambda}\n  = \\lambda ^{\\sum_{i=1}^{n} x_i} e^{-n \\lambda} \\qty ( \\prod_{i=1}^{n} x_i ! ) ^{-1}$$\nと表される。ここで\n$$g_\\lambda = \\lambda ^{\\sum_{i=1}^{n} x_i} e^{-n \\lambda}, \\quad h(x) = \\qty ( \\prod_{i=1}^{n} x_i ! ) ^{-1}$$\nとおけば、$T = \\sum_{i=1}^{n}X_i$が十分統計量であることがわかる。\n\n### 正規分布\n\n##### 一次元の十分統計量の計算\n\n$X_1, \\dots, X_n \\sim N(\\mu, 1), i.i.d.$とする。$X$の同時密度関数は\n$$\\begin{aligned}\n    f(x) & = \\frac{1}{(2\\pi)^{n/2}} \\exp \\qty [ - \\frac{1}{2} \\sum_{i=1}^{n} (x_i - \\mu)^2 ]                                                                                                      \\\\\n         & = \\frac{1}{(2\\pi)^{n/2}} \\exp \\qty [ - \\frac{1}{2} \\sum_{i=1}^{n} (x_i - \\bar{x} + \\bar{x} - \\mu)^2 ]                                                                                  \\\\\n         & = \\frac{1}{(2\\pi)^{n/2}} \\exp \\qty [ - \\frac{1}{2} \\sum_{i=1}^{n} (x_i - \\bar{x})^2  - \\sum_{i=1}^{n} (x_i - \\bar{x})(\\bar{x} - \\mu)  - \\frac{1}{2} \\sum_{i=1}^{n} (\\bar{x} - \\mu)^2 ] \\\\\n         & = \\frac{1}{(2\\pi)^{n/2}} \\exp \\qty [ - \\frac{1}{2} \\sum_{i=1}^{n} (x_i - \\bar{x})^2 - \\frac{n (\\bar{x} - \\mu)^2}{2} ]\n  \\end{aligned}$$ と表すことができる。したがって、\n$$g_\\mu = \\frac{1}{(2\\pi)^{n/2}} \\exp \\qty (- \\frac{n (\\bar{x} - \\mu)^2}{2}), \\quad\n  h(x) = \\exp \\qty (- \\frac{1}{2} \\sum_{i=1}^{n} (x_i - \\bar{x})^2)$$\nとおけば、分解定理より$\\bar{X}$が十分統計量であることがわかる。\n\n### 一様分布\n\n##### 一次元の十分統計量の計算\n\n$X_1, ..., X_n \\sim U[0, \\theta]$とする。同時密度関数は\n$$\\label{eq:uniform_pdf}\n  f_\\theta (x) = \n  \\begin{dcases}\n    \\frac{1}{\\theta^{n}}, & \\text{ if } 0 \\leq x_i \\leq \\theta, \\forall i \\\\\n    0,                    & \\text{otherwise}\n  \\end{dcases}$$\nと表される。ここで、$X_{max} = \\max_{i} X_i$とし、以下の関数を定義する。\n$$I _{[x_{max} \\leq \\theta]}(x) =\n  \\begin{dcases}\n    1, & \\text{ if } x_{max} \\leq \\theta \\\\\n    0, & \\text{ otherwise}\n  \\end{dcases}$$\n\n$$h(x) = h(x_1, \\dots, x_n) = \n  \\begin{dcases}\n    1, & \\text{ if } x_i \\geq 0, \\forall i \\\\\n    0, & \\text{ otherwise }\n  \\end{dcases}$$\n\n$x_i \\geq 0, \\forall i \\Leftrightarrow x_{max} \\leq \\theta$であることに注意すれば、[\\[eq:uniform_pdf\\]](#eq:uniform_pdf){reference-type=\"ref\"\nreference=\"eq:uniform_pdf\"}は\n$$f_{\\theta} (x) = \\frac{1}{\\theta^n} I _{[x_{max} \\leq \\theta]}(x) h(x)$$\nと書ける。したがって、\n$$g_{\\theta} = \\frac{1}{\\theta^n} I _{[x_{max} \\leq \\theta]}(x)$$\nとおけば、分解定理より$T(X) = X_{max}$が十分統計量であることがわかる。\n\n## 不偏推定量\n\n### 概要\n\n#### 不偏推定量の定義\n\n$\\hat{\\theta}$が不偏推定量であるとは、\n$$E_{\\theta} \\qty [\\hat(\\theta)(X)] = \\theta, \\quad \\forall \\theta$$\nが成り立つことである。\n\n#### 一様最小分散不偏推定量(Uniformly Minimum Variance Unbiased estimator, UMVU)の定義\n\n$\\hat{\\theta}$が不偏推定量であれば平均二乗誤差は分散に一致する。\n$$E_{\\theta} \\qty [( \\hat{\\theta} - \\theta )] = E_{\\theta} \\qty [( \\hat{\\theta} - E_{\\theta}[\\hat{\\theta}] )] = Var_{\\theta} \\qty [\\hat{\\theta}]$$\nしたがって、不偏推定量に限れば、分散を最小にする推定量が望ましい推定量となる。\n\n不偏推定量$\\hat{\\theta}^{*}$がUMVUであるとは、任意の不偏推定量$\\hat{\\theta}$について\n$$Var_{\\theta} \\qty [\\hat{\\theta}^{*}] \\leq Var_{\\theta} \\qty [\\hat{\\theta}], \\quad \\forall \\theta$$\nが成り立つことである。\n\n::: outline\n与えられた不偏推定量がUMVUであることを示す方法\nフィッシャー情報量に基づくクラメル・ラオの不等式を用いる方法\n完備十分統計量の理論を用いる方法\n:::\n\n#### フィッシャー情報量\n\n$X = (X_1, \\dots, X_n)$の同時密度関数あるいは同時確率関数を$f(x, \\theta)$で表す。\nここで、$\\theta$は1次元のパラメータとする。\nこのとき$\\theta$に関するフィッシャー情報量$I_n(\\theta)$は次式で定義される。\n$$\\begin{aligned}\n    I_n(\\theta) & = E_{\\theta} \\qty [\\qty (\\pdv{\\log f(x, \\theta)}{\\theta})^2]                        \\\\\n                & = \\int \\qty (\\frac{\\pdv{\\theta} f(x, \\theta)}{f(x, \\theta)}) ^2 f(x, \\theta) \\dd{x} \\\\\n                & = \\int \\frac{\\qty (\\pdv{\\theta} f(x, \\theta))^2}{f(x, \\theta)} \\dd{x}\n  \\end{aligned}$$\n\n$X_1, \\dots, X_n$が独立同一分布に従うときには\n$$I_n(\\theta) = nI_1(\\theta)$$ となる。\n\n#### クラメル・ラオの不等式\n\n不偏推定量の分散とフィッシャー情報量の間には次の不等式が成立する。\n$\\hat{\\theta}$を$\\theta$の不偏推定量とする。このとき\n$$Var_{\\theta} \\qty [\\hat{\\theta}] \\geq \\frac{1}{I_n(\\theta)}$$\n\n#### 不偏推定量がUMVUであることを示す\n\n##### フィッシャー情報量に基づくクラメル・ラオの不等式を用いる方法\n\n不偏推定量$\\hat{\\theta}^{*}$が\n$$Var_{\\theta} \\qty [\\hat{\\theta}^{*}] = \\frac{1}{I_n(\\theta)}, \\quad \\forall \\theta$$\nを満たせば、$\\hat{\\theta}^{*}$はUMVUである。\n\n##### 完備十分統計量の理論を用いる方法\n\n$T$を完備十分統計量とする。このとき$T$の関数である不偏推定量$\\delta^{*}(T)$は一意的に定まり\n$\\delta^{*}(T)$はUMVUとなる。\nまた、任意の不偏推定量を$\\hat{\\theta}$とするとき\n$E[\\hat{\\theta} | T]$は$\\delta^{*}(T)$に一致する。\n\n以上のことは次のように確かめられる。\n$\\hat{\\theta}$を不偏推定量とし、$T(X)$を（一般の）十分統計量とする。\n$$\\hat{\\theta}^{*}(t) = E \\qty [\\hat{\\theta}(X) | T(X) = t]$$\nとおくと、期待値の繰り返しの公式により、\n$$\\theta = E_{\\theta} \\qty [\\hat{\\theta}(X)] = E_{\\theta}^{T} \\qty [E [\\hat{\\theta}|T]] = E_{\\theta}^{T} \\qty [\\hat{\\theta}^{*}(T)]$$\nとなるから、$\\hat{\\theta}^{*}(T)$も不偏推定量であることがわかる。\nそしてラオ・ブラックウェルの定理により、$\\hat{\\theta}^{*}(T)$と$\\hat{\\theta}$が一致しない限り\n$Var_{\\theta} \\qty [\\hat{\\theta}^{*}(T)] < Var_{\\theta} \\qty [\\hat{\\theta}]$\nとなる。 [^4]\n\n次に、$T$を完備十分統計量とする。同様に\n$$\\hat{\\theta}^{*}(t) = E \\qty [\\hat{\\theta}(X) | T(X) = t]$$ とおく。\nここで、$\\tilde{\\theta}$をほかの任意の不偏推定量とする。\n$\\tilde{\\theta}$にラオ・ブラックウェルの定理を応用して\n$$\\tilde{\\theta}^{*}(t) = E \\qty [\\tilde{\\theta}(X) | T(X) = t]$$\nとおく。\nこのとき、$Var_{\\theta} [\\tilde{\\theta}] \\geq Var_{\\theta} [\\tilde{\\theta}^{*}]$である。\nところで、$g(T) = \\hat{\\theta}^{*}(T) - \\tilde{\\theta}^{*}(T)$とおくと、\n$$E_{\\theta} \\qty [g(T)] = E_{\\theta} \\qty [\\hat{\\theta}^{*}(T) - \\tilde{\\theta}^{*}(T)] = \\theta - \\theta = 0, \\quad \\forall \\theta$$\nとなる。したがって完備性の定義より$g(T) \\equiv 0$すなわち$\\hat{\\theta}^{*}(T) \\equiv \\tilde{\\theta}^{*}(T)$でなければならない。\nこのとき、$Var_{\\theta} \\qty [\\tilde{\\theta}^{*}] = Var_{\\theta} \\qty [\\hat{\\theta}^{*}]$となるから、\n$Var_{\\theta} \\qty [\\tilde{\\theta}] \\geq Var_{\\theta} \\qty [\\hat{\\theta}^{*}]$となることが示された。\n[^5]\n\n## 最尤推定量\n\n### 多項分布\n\n対数尤度関数は\n$$l(p_1, p_2, \\dots, p_k) = x_1 \\log p_1 + x_2 \\log p_2 + \\dots x_k \\log p_k + const.$$\n\nこれを、$p_1 + p_2 + \\dots + p_k = 1$の条件の下で最大化する$p_i$を見つける。\nLagrangeの未定乗数$\\lambda$を用いて、関数\n$$\\tilde{l}(p_1, p_2, \\dots, p_k, \\lambda)\n  = x_1 \\log p_1 + x_2 \\log p_2 + \\dots x_k \\log p_k - \\lambda (p_1 + p_2 + \\dots + p_k - 1)$$\nを定義する。これをパラメータで微分したものがゼロという条件から\n$$\\pdv{\\tilde{l}}{p_i} = \\frac{x_i}{p_i} - \\lambda = 0$$\nという式が得られる。これより $$\\hat{p}_i = \\frac{x_i}{\\lambda}$$\nが得られ、条件$p_1 + p_2 + \\dots + p_k = 1$から $$\\lambda = n$$\nがわかる。以上から、$p_i$の最尤推定量は $$\\hat{p_i} = \\frac{x_i}{n}$$\nとなる。\n\n## 正規分布に関する検定\n\n### 2標本の平均の検定（分散が未知の場合） {#sec:2samples_mean_test_unknown_variance}\n\n分散$\\sigma_{A}^2$と$\\sigma_{B}^2$は未知とする。 各群の標本分散\n$$\\begin{aligned}\n    s_{A}^2 = \\frac{1}{n_{A} - 1} \\sum_{i = 1}^{n_A} (X_{Ai} - \\bar{X}_{A})^2 \\\\\n    s_{B}^2 = \\frac{1}{n_{B} - 1} \\sum_{i = 1}^{n_B} (X_{Bi} - \\bar{X}_{B})^2 \n  \\end{aligned}$$ で$\\sigma_{A}^2$と$\\sigma_{B}^2$を推定する。\n共通の母分散$\\sigma_{A}^2 = \\sigma_{B}^2 = \\sigma^2$を仮定すると、\n2つの群をプールした標本分散\n$$s^2 = \\frac{(n_{A} - 1) s_{A}^2 + (n_{B} - 1) s_{B}^2}{n_{A} + n_{B} - 2}$$\n$\\sigma^2$を推定する。\n$\\frac{(n_{A} + n_{B} - 2) s^2}{\\sigma^2}$は自由度$(n_{A} + n_{B} - 2)$のカイ2乗分布に従う。\n[^6]\n共通の母分散を仮定すると$\\bar{X}_{A} - \\bar{X}_{B}$と$s^2$は互いに独立であるから、\n帰無仮説$H_0:\\delta = \\delta_0$のもとで、検定統計量\n$$T = \\frac{ \\bar{X}_{A} - \\bar{X}_{B} - \\delta_{0}}{ s \\sqrt{ \\frac{1}{n_{A}} + \\frac{1}{n_{B}}}}$$\n自由度$(n_{A} + n_{B} - 2)$の$t$分布に従う。\n\n### 1元配置分散分析\n\nモデル $$y_{ij} \\sim N(\\mu(A_i),\\sigma^2)$$\n\n一般平均 $$\\mu = \\frac{1}{a} \\sum_{i = 1}^{a} \\mu (A_i)$$\n\n因子の効果 $$\\alpha_i = \\mu (A_i) - \\mu$$\n\nこのとき、\n$$y_{ij} = \\mu + \\alpha_i + \\epsilon_{ij}, \\quad \\epsilon_{ij} \\sim N(0, \\sigma^2)$$\nと書ける。\n\n帰無仮説　$H_0: \\alpha_1 = \\cdots = \\alpha_a = 0$\\\n対立仮説　$H_1:$ =が少なくとも1つは成り立たない\\\nこの検定に**分散分析**を用いる\n\n総平方和 $${\n    S_T = \\sum_{i =1}^{a} \\sum_{j=1}^{n} (y_{ij}- \\bar{y})^2 \n    = \\sum_{i =1}^{a} \\sum_{j=1}^{n} y_{ij}^{2} - an \\bar{y}^{2}, \\quad 自由度 \\phi_T = an -1\n  }$$\n\n水準間平方和 $${\n    S_A = \\sum_{i =1}^{a} \\sum_{j=1}^{n} (\\bar{y}_{A_i}- \\bar{y})^2 \n    = n \\sum_{i =1}^{a}  (\\bar{y}_{A_i}- \\bar{y})^2 \n    = n \\sum_{i =1}^{a} \\bar{y}_{A_i}^{2} - an \\bar{y}^{2} , \\quad 自由度 \\phi_A = a -1\n  }$$\n\n誤差平方和 [^7] $${\n    S_E = \\sum_{i =1}^{a} \\sum_{j=1}^{n} (y_{ij}- \\bar{y}_{A_i})^2 = S_{T} - S_{A} , \\quad 自由度 \\phi_E = a (n-1)\n  }$$\n\n帰無仮説が成り立つとき、 $${\n    F = \\frac{V_A}{V_E} = \\frac{S_A / \\phi_A}{S_E / \\phi_E}\n  }$$\nが自由度$(\\phi_A,\\phi_E)$のF分布に従う。これから、F値を求め、$H_0$を検定する。\\\n帰無仮説のもとで、 $$\\begin{aligned}\n  \\frac{\\bar{y}_{A_i} - \\mu(A_i)}{\\sigma / \\sqrt{n}} \\sim N(0, 1)                \\\\\n  \\frac{S_{E}}{\\sigma^2} = \\phi_{E} \\frac{V_{E}}{\\sigma^2} \\sim \\chi^2(\\phi_{E}) \\\\\n\\end{aligned}$$ が成り立ち、$\\bar{y}_{A_i}$と$V_{E}$は独立であるから\n$$\\frac{\\frac{\\bar{y}_{A_i} - \\mu(A_i)}{\\sigma / \\sqrt{n}}}{\\sqrt{V_{E} / \\sigma^2}}\n  = \\frac{\\bar{y}_{A_i} - \\mu(A_i)}{\\sqrt{V_{E}/n}} \\sim t(\\phi_{E})$$\nとなる。以上から、 平均$\\mu (A_i)$の点推定 $${\n    \\hat{\\mu} (A_i) = \\bar{y}_{A_i}\n  }$$ 平均$\\mu (A_i)$の95%信頼区間 $${\n    \\bar{y}_{A_i} \\pm t_{0.025} (\\phi_E) \\sqrt{V_E / n}\n  }$$\n\n## デルタ法\n\n確率変数$X$について、平均と分散が\n$$E \\qty[X] = \\mu_{x}, \\quad V \\qty[X] = \\sigma_{x}^2$$\nのようにわかっているとする。\nこのとき、$Y = g(X)$と変数変換を行った確率変数$Y$の平均や分散を近似的に求める。\n\n##### 分散の近似\n\n$Y = g(X)$の1次までの展開は\n$$Y = g(X) \\approx g(\\mu_{x}) + g'(\\mu_{x})  (X - \\mu_{x})$$\nとなり、この両辺の分散を取ることで\n$$V[Y] \\approx \\qty (g'(\\mu_{x}))^2 V \\qty[X] = \\qty (g'(\\mu_{x}))^2 \\sigma_{x}^2$$\nとなる。\n\n##### 平均の近似\n\n$Y = g(X)$の2次までの展開は\n$$Y = g(X) \\approx g(\\mu_{x}) + g'(\\mu_{x})  (X - \\mu_{x}) \n  + \\frac{1}{2} g^{''}(\\mu_{x}) (X - \\mu_{x})^2$$\nとなるから、この両辺の平均をとることで\n$$E \\qty[Y] \\approx g(\\mu_{x}) + \\frac{1}{2} g^{''}(\\mu_{x}) V \\qty[X]\n  = g(\\mu_{x}) + \\frac{1}{2} g^{''}(\\mu_{x}) \\sigma_{x}^2$$ となる。\n\n## 指数分布族\n\n($k$母数)指数型分布族の密度関数\n$$f (x, \\theta) = h(x) \\exp \\qty ( \\sum_{j=1}^{k} T_j(x)\\psi_{j}(\\theta) - c(\\theta))$$\n\n分解定理より、$T_1, \\cdots, T_k$が$k$次元の十分統計量をなす。\n\n## 生存時間解析\n\n\n[^2]: 言葉で表すと、$$十分統計量で条件付けた推定量の二乗誤差 \\leq 元の推定量の二乗誤差$$となる。\n    つまり、十分統計量で条件付けると、推定量の二乗誤差は小さくなる。\n\n[^3]: 次のように言うこともできる。 任意の関数$h_1(T), h_2(T)$に対し、\n    $$E_{\\theta} \\qty [h_1(T)] = E_{\\theta} \\qty [h_1(T)], \\forall \\theta \\Rightarrow h_1(T) \\equiv h_2(T)$$\n\n[^4]: つまり、十分統計量で条件付けた不偏推定量は条件付けない時と比べて分散が小さくなる。\n\n[^5]: 任意の不偏推定量$\\hat{\\theta}$を完備十分統計量で条件付けると、\n    条件付ける前の不偏推定量がどんなものであってもある不偏推定量$\\hat{\\theta}^{*}$になる。\n    $\\hat{\\theta}^{*}$は条件付けないときの不偏推定量と比べて分散が小さくなる。\n    つまり、任意の不偏推定量より分散が小さくなる。\n\n[^6]: $X \\sim \\chi^2(n)$のとき、$E \\qty[X/n] = 1, V \\qty [X/n] = \\frac{2}{n}$となる。\n    つまり、自由度が大きい方が分散が小さくなる。\n    以上から、共通の分散を仮定しているとき、$s_{A}^2$(自由度$n_{A} - 1$)や$s_{B}^2$(自由度$n_{B} - 1$)より\n    $s^2$(自由度$n_{A} + n_{B} - 2$)を$\\sigma^2$の推定量として使用したほうが分散が小さくなって良いことがわかる。\n\n[^7]: $V_{E} = \\frac{S_{E}}{\\phi_{E}}$は[6.1](#sec:2samples_mean_test_unknown_variance){reference-type=\"ref\"\n    reference=\"sec:2samples_mean_test_unknown_variance\"}でのプールした標本分散に対応する量である。\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":true,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"css":["../../_styles/style.css","style.css"],"number-sections":true,"output-file":"index.html"},"language":{},"metadata":{"lang":"ja","fig-responsive":true,"quarto-version":"1.2.280","theme":"flatly","bibliography":["../../references.bib"],"title":"数理統計学"},"extensions":{"book":{"multiFile":true}}}}}
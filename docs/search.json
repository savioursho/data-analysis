[
  {
    "objectID": "01-EDA/R-for-data-science.html",
    "href": "01-EDA/R-for-data-science.html",
    "title": "R for Data Science Exploratory Data Analysis",
    "section": "",
    "text": "データに関する問いを生み出し\n可視化、変換、モデリングによってその答えを探し\nそれによって学んだことを使って、問いを精緻化したり、新しい問いを生み出したりすること\n\n\n\n\n\n\nEDAの目的はデータを理解すること。\nこれを達成するためのいちばん簡単な方法は問いを使うこと\n2つのタイプの問いが有用である:\n\n各変数にはどのような変動があるのか？\n変数の間にはどのような共変動があるのか？\n\n\n\n\n\n\n\n\nカテゴリカルデータ\n\n棒グラフ\n\n連続値データ\n\nヒストグラム\n複数のヒストグラムを重ねたいときはFrequency Polygonがおすすめ\n\n\n\n\n\n\n\n\n\nどの値が最も多い？それはなぜ？\nどの値が一番少ない？それはなぜ？それは予想通り？\n特徴的なパターンはある？それはどのように説明できる？\n\n\n\n\n\nクラスターの存在はデータの中のサブグループの存在を示唆する。\nサブグループについて理解するための問い:\n\n各クラスター内の観測値はどのように似ている？\n各クラスター間の観測値はどのように異なっている？\nクラスターをどのように特徴づけられる？\n\n\n\n\n\n\n\n外れ値の有無で分析結果が変わる？\n\nYes: 外れ値の原因を明らかにして、報告書に記載する\nNo: 外れ値を欠損に置き換えて分析を進める\n\n\n\n\n\n\n\n\n\n\n\nカテゴリごとのヒストグラム\nカテゴリごとのdensity plot\n\nカテゴリごとに度数が大きく異る場合に有用\n\nカテゴリごとの箱ひげ図\n\n\n\n\n\nクロス集計した度数をバブルチャートのように表示\nクロス集計した度数をヒートマップで表示\n\nseriationで行・列を並び替えてパターンを発見することもできる\n\n\n\n\n\n\n散布図\n\n点が重なるときは透明度を調整\n\n2dヒストグラム\n\n散布図で点が重なってしまうときにも有用\n\n片方をbinningして箱ひげ図"
  },
  {
    "objectID": "01-EDA/seriation/seriation.html",
    "href": "01-EDA/seriation/seriation.html",
    "title": "データ分析情報まとめ",
    "section": "",
    "text": "import seaborn as sns\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport gc\nimport os\n\n\n# 自作関数のインポート\n\nfrom seriation import save_clustermap_all\n\nsns.clustermapはデフォルトではmethod=\"single\"(最近隣法), metric=\"euclidean\"でクラスタリングを行う\n\n\n\n下三角行列+ノイズのデータフレームを作成\n\nsize = 15\ndf_tri = pd.DataFrame(np.tri(size)) * 10 + np.random.randn(size, size)\n\nsns.heatmap(df_tri, cmap=\"coolwarm\")\n\n<AxesSubplot: >\n\n\n\n\n\n行と列をシャッフル\n\ndf_tri_shufful = df_tri.sample(frac=1.0).T.sample(frac=1.0)\nsns.heatmap(df_tri_shufful, cmap=\"coolwarm\")\n\n<AxesSubplot: >\n\n\n\n\n\n様々なmetricとmethodの組み合わせでclustermap\n\npath_fig = \"./fig/tri.png\"\nsave_clustermap_all(df_tri_shufful, path_fig)\n\n\n\n\ntri\n\n\n\nsns.get_dataset_names()\n\n['anagrams',\n 'anscombe',\n 'attention',\n 'brain_networks',\n 'car_crashes',\n 'diamonds',\n 'dots',\n 'dowjones',\n 'exercise',\n 'flights',\n 'fmri',\n 'geyser',\n 'glue',\n 'healthexp',\n 'iris',\n 'mpg',\n 'penguins',\n 'planets',\n 'seaice',\n 'taxis',\n 'tips',\n 'titanic']\n\n\n\n\n\n\ndf_diamonds: pd.DataFrame = sns.load_dataset(\"diamonds\")\ndf_diamonds.head()\n\n\n\n\n\n  \n    \n      \n      carat\n      cut\n      color\n      clarity\n      depth\n      table\n      price\n      x\n      y\n      z\n    \n  \n  \n    \n      0\n      0.23\n      Ideal\n      E\n      SI2\n      61.5\n      55.0\n      326\n      3.95\n      3.98\n      2.43\n    \n    \n      1\n      0.21\n      Premium\n      E\n      SI1\n      59.8\n      61.0\n      326\n      3.89\n      3.84\n      2.31\n    \n    \n      2\n      0.23\n      Good\n      E\n      VS1\n      56.9\n      65.0\n      327\n      4.05\n      4.07\n      2.31\n    \n    \n      3\n      0.29\n      Premium\n      I\n      VS2\n      62.4\n      58.0\n      334\n      4.20\n      4.23\n      2.63\n    \n    \n      4\n      0.31\n      Good\n      J\n      SI2\n      63.3\n      58.0\n      335\n      4.34\n      4.35\n      2.75\n    \n  \n\n\n\n\n\ndf_diamonds.describe(include=\"all\")\n\n\n\n\n\n  \n    \n      \n      carat\n      cut\n      color\n      clarity\n      depth\n      table\n      price\n      x\n      y\n      z\n    \n  \n  \n    \n      count\n      53940.000000\n      53940\n      53940\n      53940\n      53940.000000\n      53940.000000\n      53940.000000\n      53940.000000\n      53940.000000\n      53940.000000\n    \n    \n      unique\n      NaN\n      5\n      7\n      8\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      top\n      NaN\n      Ideal\n      G\n      SI1\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      freq\n      NaN\n      21551\n      11292\n      13065\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      mean\n      0.797940\n      NaN\n      NaN\n      NaN\n      61.749405\n      57.457184\n      3932.799722\n      5.731157\n      5.734526\n      3.538734\n    \n    \n      std\n      0.474011\n      NaN\n      NaN\n      NaN\n      1.432621\n      2.234491\n      3989.439738\n      1.121761\n      1.142135\n      0.705699\n    \n    \n      min\n      0.200000\n      NaN\n      NaN\n      NaN\n      43.000000\n      43.000000\n      326.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      25%\n      0.400000\n      NaN\n      NaN\n      NaN\n      61.000000\n      56.000000\n      950.000000\n      4.710000\n      4.720000\n      2.910000\n    \n    \n      50%\n      0.700000\n      NaN\n      NaN\n      NaN\n      61.800000\n      57.000000\n      2401.000000\n      5.700000\n      5.710000\n      3.530000\n    \n    \n      75%\n      1.040000\n      NaN\n      NaN\n      NaN\n      62.500000\n      59.000000\n      5324.250000\n      6.540000\n      6.540000\n      4.040000\n    \n    \n      max\n      5.010000\n      NaN\n      NaN\n      NaN\n      79.000000\n      95.000000\n      18823.000000\n      10.740000\n      58.900000\n      31.800000\n    \n  \n\n\n\n\ncolorとclarityをクロス集計してヒートマップを作成する\n\n(\n    df_diamonds[[\"color\", \"clarity\"]]\n    .value_counts()\n    .unstack()\n    .pipe(sns.heatmap, cmap=\"coolwarm\")\n)\n\n<AxesSubplot: xlabel='clarity', ylabel='color'>\n\n\n\n\n\n\n(\n    df_diamonds[[\"color\", \"clarity\"]]\n    .value_counts()\n    .unstack()\n    .pipe(sns.clustermap, cmap=\"coolwarm\", figsize=(5, 5))\n)\n\n<seaborn.matrix.ClusterGrid at 0x7fb7b46a3c70>\n\n\n\n\n\n\npath_fig = \"./fig/diamonds.png\"\n\ndf_diamonds_crosstab = (\n    df_diamonds[[\"color\", \"clarity\"]]\n    .value_counts()\n    .unstack()\n)\nsave_clustermap_all(df_diamonds_crosstab, path_fig)\n\n/usr/local/lib/python3.10/dist-packages/seaborn/matrix.py:615: UserWarning: Attempting to set identical low and high xlims makes transformation singular; automatically expanding.\n  ax.set_xlim(0, max_dependent_coord * 1.05)\n/usr/local/lib/python3.10/dist-packages/seaborn/matrix.py:623: UserWarning: Attempting to set identical low and high ylims makes transformation singular; automatically expanding.\n  ax.set_ylim(0, max_dependent_coord * 1.05)\n\n\n\n\n\n\n\n\ndiamonds\n\n\n\n\n\n\ndf_iris: pd.DataFrame = sns.load_dataset(\"iris\")\ndf_iris.head()\n\n\n\n\n\n  \n    \n      \n      sepal_length\n      sepal_width\n      petal_length\n      petal_width\n      species\n    \n  \n  \n    \n      0\n      5.1\n      3.5\n      1.4\n      0.2\n      setosa\n    \n    \n      1\n      4.9\n      3.0\n      1.4\n      0.2\n      setosa\n    \n    \n      2\n      4.7\n      3.2\n      1.3\n      0.2\n      setosa\n    \n    \n      3\n      4.6\n      3.1\n      1.5\n      0.2\n      setosa\n    \n    \n      4\n      5.0\n      3.6\n      1.4\n      0.2\n      setosa\n    \n  \n\n\n\n\n\ndf_iris.describe()\n\n\n\n\n\n  \n    \n      \n      sepal_length\n      sepal_width\n      petal_length\n      petal_width\n    \n  \n  \n    \n      count\n      150.000000\n      150.000000\n      150.000000\n      150.000000\n    \n    \n      mean\n      5.843333\n      3.057333\n      3.758000\n      1.199333\n    \n    \n      std\n      0.828066\n      0.435866\n      1.765298\n      0.762238\n    \n    \n      min\n      4.300000\n      2.000000\n      1.000000\n      0.100000\n    \n    \n      25%\n      5.100000\n      2.800000\n      1.600000\n      0.300000\n    \n    \n      50%\n      5.800000\n      3.000000\n      4.350000\n      1.300000\n    \n    \n      75%\n      6.400000\n      3.300000\n      5.100000\n      1.800000\n    \n    \n      max\n      7.900000\n      4.400000\n      6.900000\n      2.500000\n    \n  \n\n\n\n\n\ndf_iris_corr = df_iris.select_dtypes(\"number\").corr()\nsns.heatmap(df_iris_corr, cmap=\"coolwarm\")\n\n<AxesSubplot: >\n\n\n\n\n\n\ndf_iris_corr = df_iris.select_dtypes(\"number\").corr()\nsns.clustermap(\n    df_iris_corr,\n    cmap=\"coolwarm\",\n    method=\"single\",\n    metric=\"euclidean\",\n    figsize=(5, 5),\n)\n\n<seaborn.matrix.ClusterGrid at 0x7fb7ac17f130>\n\n\n\n\n\n\npath_fig = \"./fig/iris.png\"\nsave_clustermap_all(df_iris_corr, path_fig)\n\n\n\n\niris\n\n\n\n\n\n\ndf_tips: pd.DataFrame = sns.load_dataset(\"tips\")\ndf_tips.head()\n\n\n\n\n\n  \n    \n      \n      total_bill\n      tip\n      sex\n      smoker\n      day\n      time\n      size\n    \n  \n  \n    \n      0\n      16.99\n      1.01\n      Female\n      No\n      Sun\n      Dinner\n      2\n    \n    \n      1\n      10.34\n      1.66\n      Male\n      No\n      Sun\n      Dinner\n      3\n    \n    \n      2\n      21.01\n      3.50\n      Male\n      No\n      Sun\n      Dinner\n      3\n    \n    \n      3\n      23.68\n      3.31\n      Male\n      No\n      Sun\n      Dinner\n      2\n    \n    \n      4\n      24.59\n      3.61\n      Female\n      No\n      Sun\n      Dinner\n      4\n    \n  \n\n\n\n\n\ndf_tips.describe(include=\"all\")\n\n\n\n\n\n  \n    \n      \n      total_bill\n      tip\n      sex\n      smoker\n      day\n      time\n      size\n    \n  \n  \n    \n      count\n      244.000000\n      244.000000\n      244\n      244\n      244\n      244\n      244.000000\n    \n    \n      unique\n      NaN\n      NaN\n      2\n      2\n      4\n      2\n      NaN\n    \n    \n      top\n      NaN\n      NaN\n      Male\n      No\n      Sat\n      Dinner\n      NaN\n    \n    \n      freq\n      NaN\n      NaN\n      157\n      151\n      87\n      176\n      NaN\n    \n    \n      mean\n      19.785943\n      2.998279\n      NaN\n      NaN\n      NaN\n      NaN\n      2.569672\n    \n    \n      std\n      8.902412\n      1.383638\n      NaN\n      NaN\n      NaN\n      NaN\n      0.951100\n    \n    \n      min\n      3.070000\n      1.000000\n      NaN\n      NaN\n      NaN\n      NaN\n      1.000000\n    \n    \n      25%\n      13.347500\n      2.000000\n      NaN\n      NaN\n      NaN\n      NaN\n      2.000000\n    \n    \n      50%\n      17.795000\n      2.900000\n      NaN\n      NaN\n      NaN\n      NaN\n      2.000000\n    \n    \n      75%\n      24.127500\n      3.562500\n      NaN\n      NaN\n      NaN\n      NaN\n      3.000000\n    \n    \n      max\n      50.810000\n      10.000000\n      NaN\n      NaN\n      NaN\n      NaN\n      6.000000\n    \n  \n\n\n\n\n性別x喫煙者と曜日x時間帯でチップの額をクロス集計したものをヒートマップでみてみる\n\n(\n    df_tips.pivot_table(\n        columns=[\"day\", \"time\"],\n        index=[\"sex\", \"smoker\"],\n        values=\"tip\",\n        aggfunc=\"mean\",\n        fill_value=0,\n    ).pipe(sns.heatmap, cmap=\"coolwarm\", annot=True)\n)\n\n<AxesSubplot: xlabel='day-time', ylabel='sex-smoker'>\n\n\n\n\n\n\n(\n    df_tips.pivot_table(\n        columns=[\"day\", \"time\"],\n        index=[\"sex\", \"smoker\"],\n        values=\"tip\",\n        aggfunc=\"mean\",\n        fill_value=0,\n    ).pipe(sns.clustermap, figsize=(5, 5), cmap=\"coolwarm\", annot=True)\n)\n\n<seaborn.matrix.ClusterGrid at 0x7fb7ac2125c0>\n\n\n\n\n\n\n人の属性の軸(sex-smoker)\n\n性別よりも喫煙者か否かのほうが関係が深い\n\n時間の軸(day-time)\n\nDinnerはtipが多く、Lunchは少ない傾向にあるが、木曜日はその逆になっている"
  },
  {
    "objectID": "02-probrem-solving/professional-problem-solving-skills-and-techniques/00-preface.html",
    "href": "02-probrem-solving/professional-problem-solving-skills-and-techniques/00-preface.html",
    "title": "データ分析情報まとめ",
    "section": "",
    "text": "ポジメン: ポジティブ・メンタリティ\n\n決してあきらめず前向きに物事を捉える\n\nロジシン: ロジカルシンキング\n\n論理的に考える\n\nパラテン: パラダイム転換\n\n従来の枠組みからの転換を目指す\n\n\n\n\n\n\n2つの思考\n\nゼロベース思考\n仮説思考\n\n2つの技術\n\nMECE\nロジックツリー\n\n1つのプロセス\n\nソリューション・システム"
  },
  {
    "objectID": "02-probrem-solving/professional-problem-solving-skills-and-techniques/01-thinking.html",
    "href": "02-probrem-solving/professional-problem-solving-skills-and-techniques/01-thinking.html",
    "title": "データ分析情報まとめ",
    "section": "",
    "text": "ゼロベース思考\n\nゼロベースで物事を考える=「既成の枠」を取り外す\n\n仮説思考\n\n常にその時点での結論を持ってアクションを起こす\n\n\n\n\n\n\n\ngraph LR\nA[わかること] -- 大きな溝 --> B[実行できること] -- 大きな溝 --> C[結果がうまくいくこと]\n\n\n\n\n\n図 1: ビジネスでは、結果がうまくいかなければバリューはない。\n\n\n\n\nビジネスを成功に導く = 「結果がうまくいく」ための思考がゼロベース思考と仮説思考\n\n\n\n<ゼロベース思考>の2つのポイント\n\n\n自分の狭い枠の中で否定に走らない\n顧客にとっての価値を考える\n\n\n\n\n<ゼロベース思考>の妨げになるのは「既成の枠」\n「この課題を解決するための具体策はある」という前提でゼロベースから考える\n\n\n既存の枠で考える従来どおりの思考\n\n枠の外にある解決策を見落とす\n枠の中の否定的要素を列挙してしまう\n\nゼロベース思考\n\n自分の狭い枠を超えて考える\n解決策を見出す可能性が高まる\n\n\n\n\n\n\n自分の立場だけで問題をとらえるのではなく、 「顧客にとっての価値」を考え、実行に移すことが大事\n\n\n自分の立場だけで問題をとらえると「既成の枠」から抜け出せない\n多くの創業者がビジネスを立ち上げることに成功した理由は、自身が開発者であるとどう似にユーザーでもあったから\n\nこのため、「顧客にとっての価値」を考え、貫くことができた\n\n大企業になると「顧客にとっての価値」を考え、貫くことができない\n\n規模の拡大により消費者との距離が遠くなるため\n\n\n\n\n\nZARAは<ゼロベース思考>を徹底することで、業界の常識を破り、成功した\n\n\n競合他社との違い\n\n徹底的な自前主義\n在庫切れを奨励。顧客の来店を促す\n承認の約半数を自社工場で製造。\n\n<ゼロベース思考>の徹底\n\nビジネスシステム全体を<ゼロベース思考>で構想\n構想通りのオペレーションを現場で徹底\n顧客と製造の間のコミュニケーションがスムーズかつスピーディ\n\n\n\nドトールは<ゼロベース思考>を用いてコーヒー外飲市場を切り開いた\n\n\nドトールの成功\n\n低価格で美味しいコーヒーを提供\n1日数百人もの来店\n\n<ゼロベース思考>\n\n「喫茶店に1日数百人も入らない」を取り払う\n\n単価を低くすることを実現\n\n「1日に1000杯もフレッシュなコーヒーを瞬時に供給できない」を取り払う\n\n海外の優れたコーヒーマシンを導入\n\n\n\n\n\n\n\n\n\n液晶=電卓という枠組みを取り払い、液晶そのものをキーデバイス化\n「オンリーワンを積み重ねてナンバーワンになる」の基本姿勢\n「環境先進企業」としてのブランド形成\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<仮説思考>の3つのポイント\n\n\nアクションに結び付く議論を常に持つ — 結論の仮説\n結論に導く背後の理由やメカニズムを考える — 理由の仮説\n「ベスト」を考えるよりも「ベター」を実行する — スピードを重視\n\n\n\n\nなにがなんでも結論を出すことが仮説思考の始まり\nビジネスでは1つの具体的結論が100の評論に勝る\n\n\nまず結論を出す\n出した結論に対して SO WHAT? (だから何なの) を繰り返す\nアクションに結び付く結論を出す\n\n最初は的を外していても、常にアクションに結び付く結論を持つをいうことを心がけると精度は上がる。\n\n\n\n体重が増えた\nSO WHAT?\n\n体重が増えて血圧も上がりやすくなった\n痩せないと健康によくない\n運動をする\nスポーツクラブへ週3回行く（具体的なアクションにつながる結論）\n\n\n\n\n\n\n\n\n\n\n\ngraph TD\nA[背後のメカニズムの把握] -- メカニズムがわかればアクションに結び付く結論が出せる --> B[アクションに結び付く結論を持つ]\nB -- アクションに結び付く結論を持てば背後のメカニズムを自然に考えられる --> A\n\n\n\n\n\n図 2: 「背後のメカニズムの把握」と「アクションに結び付く結論を持つ」は鶏と卵の関係\n\n\n\n\n\n背後のメカニズムを把握していれば\n\n軌道修正が容易\n要と思われるものの裏をとれる\n\n\n\n\n課題：「家庭用ゴキブリ殺虫剤の年間市場を10分以内に推定せよ」\n\n方法1: ゴキブリ殺虫剤の市場規模に関する調査資料を探索する\n\n10分以内では困難\n成長が始まったばかりで市場データがない場合も困難\n\n方法2: 自分で市場規模を推定する\n\n方法1が困難なときはこちらをせざるを得ない場合がある\n\n\n推定例:\n最も簡単な方法は 市場規模 = 1世帯あたりの年間利用金額 × カバー世帯数 によって推定する方法\n\n全国5000万世帯\n1世帯あたり年間2000円\n最大 2000 円/世帯 × 5000万世帯 = 1000億円\n\n全世帯が使用するのは現実的でない\nファミリー世帯が需要の中心として、ファミリー世帯が占める割合を推定する\n\n新聞などで発表される平均世帯数は3人\n4人家族か1人暮らしかに限定すると、3人 = 4人 × x% + 1人 × (100 - x)% を解いてファミリー世帯は約70%\n北の地域や新しいマンションでは使用しないとし、使用する世帯はファミリー世帯の半分とすると 70% × 50% = 35%\n全世帯の約3分の1の 1700世帯 が使用世帯となる\n\n年間使用金額を推定する\n\nエアゾールタイプは約500円、年間一本と仮定\nベイト剤タイプは一個50円、6カ月使用、ひと月4個使用と仮定。 50円 × 4個 × 6カ月 = 1200円\nエアゾールとベイト剤を足して 1700円。約2000円\n世帯数と金額をかけ合わせて 平均 2000 円/世帯 × 1700万世帯 = 340億円\n\n現時点でのアクションに結び付く結論:\n「家庭用ゴキブリ殺虫剤市場は340億円程度の成熟市場で成長の見込みが低く、新規参入を検討するほど市場に魅力がない」\n\n\nその時点での結論を出すこと自体が、後にさらに考えを深め、より良い解決策をだすことに結び付く\n\n\nもし根拠が希薄で不安のあるすうじがあれば、そこだけを検証すれば格段に精度が上がるはず\nメカニズムに関する仮説が納得のいくものであるが、実際の数字より多い額が出てしまったとき\n\n仮説が間違っているかもしれない\n市場が発展途上の場合は、ポテンシャル市場の額を表しており、企業が十分に市場をカバーしていないとも考えられる\n\n\n\n\n課題: 「日本国内における自転車の年間市場を推定し、新規参入の是非に関する結論を出しなさい」\n推定例:\n自転車に乗る人は小学生から60歳までと仮定。 平均寿命80歳、各年齢の人口が同じ数だとすると、自転車に乗る人は大体 50/80 くらい。 全人口が1億3000万人。 1人1台とする。\n1億3000万人 × 5/8 = 8125万人\n6年に1回買い替えると予想\n市場規模 8125万人 × 1回/6年 = 1350万台\n平均価格を1万5000円とすると\n1.5万円 × 1350万台 = 2025億円\n現時点での結論:\n「自転車市場は市場性を評価すると新規参入を検討する価値は十分にある。それに当たっては電動アシスト自転車の普及版も視野に入れた市場を考えるべきである」\n別の推定例:\n年代別の購入・買い替え頻度\n\n幼稚園児から小学生(3~12歳): 3年で買い替え\n中学生から大学生(13~22歳): 5年買い替え\n社会人(23~60歳): 10年で買い替え\n高齢者(61~80歳): 買い替えをしない\n\n年代別の人口分布が均一、平均寿命を80歳とする。 それぞれの年代の人数に1年以内に購入する確率をかけたものを足し上げる\n{ (10 × 1/3 ) + ( 10 × 1/5 ) + (38 × 1/10) + (20 × 0) } × ( 1億3000万 ÷ 80 ) = 1480万台\n市場規模 1.5万円 × 1480万台 = 2220億円\n現時点での結論:\n「自転車市場は市場性を評価すると新規参入を検討する価値は十分にある。自転車市場を細分化し、高付加価値セグメンテーションをメインターゲットに市場参入すべきである」\n\n\n推定した数字が違っていることがわかっても、そこの数字だけを調べて計算しなおす\n\nゼロから調べるよりも効率的\n\n一応出した結論に基づいてアクションプランを立て、同時に裏を取ることができる（分析とアクションの同時並行）\n\n\n\n\n\n実行することにより、今よりもベターな状況が想定されれば、とにかく実行に移す\n\n\nビジネスでは絶対的な正解はない\nベターな解決策を見つけたらすぐに実行する\nどんどん軌道修正していく\n\n\nベターな解決策で現場を動かすほうが効率的\n\n\nベターな解決策でも現場を動かし始めると精度の高いすぐ役立つ情報が自動的に入ってくる\nベターな解決策が見えたらすぐに実行に移し、試行錯誤を重ねる\nベストは最後に見えてくる\n\n\n\n\n\ngraph LR\nsubgraph 最初の段階\n    A1[現時点での結論] --> B1[検証]\n    B1 --> C1[実行]\nend\nsubgraph 次の段階\n    A2[現時点での結論] --> B2[検証]\n    B2 --> C2[実行]\nend\nsubgraph その次の段階\n    A3[現時点での結論] --> B3[検証]\n    B3 --> C3[実行]\nend\n\nC1 --> A2 \nC2 --> A3 \nC3 --> D[究極の解]\n\n\n\n\n\n\n\n\n\n\n\nビジネスの現場では、時間と環境の変化にしたがって解決策が変化していく\n6割レベルの情報が集まったら一度方向性の判断を行う\n\n\n初めにアクションに結び付く結論を言い、その結論に導く理由を説明する"
  },
  {
    "objectID": "02-probrem-solving/professional-problem-solving-skills-and-techniques/02-thechniques.html",
    "href": "02-probrem-solving/professional-problem-solving-skills-and-techniques/02-thechniques.html",
    "title": "データ分析情報まとめ",
    "section": "",
    "text": "第1章の2つの思考は問題解決のための基本態度\n第2章の2つの技術は問題解決のための基本スキル\n\n\n\n\n\n\n戦略を考えるうえで3つの動きを「モレ」なくカバーすることの大切さ\n戦略を実行するうえで「広がり」を押さえ「具体化」することの重要さ\n\nをスーパードライの誕生の背景を追いながら考える\n\n\n\n\n一般的な見方:\n\n値上げや嗜好の多様化によりビール市場は成熟してしまった\n\n消費者レベルで掘り下げた見方:\n\n「生化」や「缶化」といった構造的変化はすでに進行していた\n従来の生ビールでは十分に満足できない「ドライな生」を求めるニーズ浮上しようとしていた\n\n\n\n\n\n\nスーパードライの出現前の競合の状況:\n\nビール業界は「成熟市場」だという「既成の枠」にはまり込む\n「味」からかけ離れたところで競争が繰り広げられる\n\n消費者にとってはただ騒々しいだけ\n当時のアサヒビールもその慣習に追随していただけだった\n\n\n\n\n\nアサヒビールのシェアは下落\nコスト削減のため二流の原材料を使用\n在庫がダブつき、日付の古いおいしくないビールが店頭にならぶ\n\n\n\n\n\n\n\n\n\ngraph TB\nsubgraph 過去の悪循環\n    A[店頭での日付が古い] --> B[飲んでもおいしくない]\n    B --> C[次から買わない]\n    C --> D[流通がダブつく]\n    D --> A\nend\nsubgraph 現在の良循環\n    A2[店頭での日付が新しくフレッシュ] --> B2[飲めば新鮮でおいしい]\n    B2 --> C2[また次に買う]\n    C2 --> D2[流通がダブつかない]\n    D2 --> A2\nend\n\n\n\n\n\n図 1: スーパードライの「フレッシュ・ローテーション」革命\n\n\n\n\n\n戦略は消費者が支配する\n市場(顧客)への深い洞察から新たなビジネスの種は生まれる\n\n\n新たなコンセプトの生ビールで、大きな消費者の「モレ」をとらえた\n日本のビールを「保存」飲料から「生鮮」飲料に変えた\n\n\n\n\n企業経営においては競合の動きや自社の業績が目に入りやすい\n一番遠く見落としがちなのが、市場の動き\n市場の動きをとらえ、「顧客にとっての価値」を考えることを忘れない\n\n\n\n\n\nアサヒビールのスーパードライ戦略の基本:\n\n「おいしいビールを消費者に提供する」\n\nそのための3つの要件\n\n「いいものを作る」\n「いいものを伝える」\n「いいものを維持する」\n\n\n\n\n\n\n\ngraph LR\nA[おいしい生ビールを消費者に提供する]\n\nB1[いいものを作る]\nB2[いいものを伝える]\nB3[いいものを維持する]\n\nsubgraph 主要な打ち手\nC11[\"消費者の求める「軽くてのど越しがいいドライ」な生ビールを開発する\"]\nC12[\"最高の原材料を使って品質が最高のビールを生産する\"]\n\nC21[\"100万人の消費者に135mlの試飲缶をトライしてもらうイベント・キャンペーンを実施\"]\nC22[\"TV-CMを中心とした大々的広告キャンペーンを実施する\"]\nC23[\"酒販店にフレッシュ・ローテーションを徹底して伝えるインナー・キャンペーンを実施する\"]\n\nC31[\"工場出荷から8日以内に店頭に並べ、<br>\n3カ月経ったものは店頭に置かないフレッシュ・ローテーションを徹底する\"]\nC32[\"マーケットレディーを組織化し、<br>\n酒販店からの商品クレームや売り上げ情報を本社にフィードバックする\"]\nend\n\nA --> B1\nA --> B2\nA --> B3\n\nB1 --> C11\nB1 --> C12\n\nB2 --> C21\nB2 --> C22\nB2 --> C23\n\nB3 --> C31\nB3 --> C32\n\n\n\n\n\n図 2: スーパードライの戦略展開\n\n\n\n\n\n\n\n\nMutually Exclusive Collectively Exhaustive\n\n\n＜MECE＞が重要な理由は、経営資源に制限がある限り、大きなモレやダブりは ビジネスの効果・効率を著しく阻害するため。\n＜MECE＞を活用する上での3つのポイント\n\nモレによって的を外していないか？\nダブりによって効率を阻害していないか？\n＜MECE＞でとらえ、最後に優先順位をつけているか？\n\n\n\nできるだけモレを出さないように、 <ゼロベース思考>で物事を大きくとらえることから始めるべき。\n\n\n\nダブりが生じていると、 資源配分上効率が悪くなるだけでなく、 受け手に対して混乱を与える場合がある。\n\n\n\n\n＜MECE＞でとらえられるようになったら、 必ず優先順位をつける。\nどんなに＜MECE＞になっていても、すべてをカバーした網羅的な解決策は、 何も言っていないのと同じことである。\n\n特に＜MECE＞の重要性が増すのは、 戦略を立案するとき。 貴重な経営資源の配分に当たっては、 できるだけモレやダブりを最小限に抑えながら優先順位をつけることが、 企業の意思決定者にとって肝要となる。\n＜MECE＞は網羅性を追及するための分解ツールとしては強力だが、 その先の経営資源配分の優先順位付けに失敗すれば意味がない。\n\n\n\n\n\n世の中のフレームワークの多くは＜MECE＞の応用である。\n\n\n\n\n顧客（Customer）、競合（Comperitor）、自社（Company）およびチャネル（Channel）\n\n戦略は単純明快なほどいい。 なぜなら、あまり複雑だと実行が困難になるから。\n\n\n\n\n製品・サービスが開発されてから市場に出るまでの 付加価値の流れを時間軸で＜MECE＞に整理したフレームワークを ビジネス・システムとかバリュー・デリバリー・システムとか呼ぶ。\n\n<ゼロベース思考>で「顧客にとっての最大の価値を提供する」視点で自社のビジネスの流れを見直すと、 現状とのギャップが明確になる。\n\n\n\n\nProduct, Price, Place, Promotion\n\n\n\n\n\nX軸に相対マーケットシェア、 Y軸に市場成長率をとって各事業をプロット\n資源配分を考えるときに使う\n\n\n\n\n\n\nロジックツリーは以下の3つの点で優れている\n\nモレやダブりを未然にチェックできる\n原因・解決策を具体的に落とし込める\n各内容の因果関係を明らかにできる\n\n\n\n\n問題を解決するときは、 根っこの具体的原因を突き止めることが重要\n\nロジックツリーを使って、 根っこの具体的原因を突き止めるには、 とにかくWHY?を自問自答し続けることが必要。\nWHY?を続けて問題を深堀すれば、 問題のの裏返しの解も通用するものになるかもしれない。\n真相が解明されてきたら、 解決策具体化のためのロジックツリー によりいろいろな解決策のオプションを考える。"
  },
  {
    "objectID": "02-probrem-solving/professional-problem-solving-skills-and-techniques/index.html",
    "href": "02-probrem-solving/professional-problem-solving-skills-and-techniques/index.html",
    "title": "問題解決プロフェッショナル 「思考と技術」",
    "section": "",
    "text": "リンク"
  },
  {
    "objectID": "03-statistics/causal-inference/index.html",
    "href": "03-statistics/causal-inference/index.html",
    "title": "データ分析情報まとめ",
    "section": "",
    "text": "因果推論"
  },
  {
    "objectID": "03-statistics/mathematical-statistics/index.html",
    "href": "03-statistics/mathematical-statistics/index.html",
    "title": "数理統計学",
    "section": "",
    "text": "\\[\n% physicsを使えるようにする\n\\require{physics}\n\\]\n\n\n\n\n\n\n\n表記: \\(Bin(n, p)\\)\nパラメータ\n\n試行回数: \\(n\\)\n成功確率: \\(p\\)\n\n確率関数: \\[p (k) = \\binom{n}{k} p^k (1-p)^{n-k}\\]\n確率母関数: \\[G (s) = (1 + p (s - 1))^n\\]\n期待値・分散: \\[E \\qty [ X] = np, \\quad Var \\qty [ X ] = np (1 - p)\\]\n再生性:\n\\(X_1 \\sim Bin(n_1, p), X_2 \\sim Bin(n_2, p)\\) のとき、 \\(Y = X_1 + X_2\\)は\\(Bin(n_1 + n_2, p)\\)に従う。\n\n\n\n\n\n\n\n表記: \\(Po(\\lambda)\\)\nパラメータ\n\n平均: \\(\\lambda\\)\n\n確率関数: \\[p (k) = \\frac{\\lambda^{k}}{k!} e^{-\\lambda}\\]\n期待値・分散: \\[E \\qty [ X] = \\lambda , \\quad Var \\qty [ X ] = \\lambda\\]\n\n\n\n\n\n\n\n意味:\nベルヌーイ試行の成功確率を \\(p\\) とし、 確率変数 \\(X\\) を1回成功するまでの失敗の回数とする。 \\(X\\) はパラメータ \\(p\\) の幾何分布に従う。\n表記: \\(Ge(p)\\)\nパラメータ\n\n成功確率: \\(p\\)\n\n確率関数: \\[p (k) =  p(1-p)^{k}\\]\n確率母関数: \\[G (s) = \\frac{p}{1 - qs} = \\qty ( 1 - (s - 1) \\frac{q}{p})^{-1}\\]\n期待値・分散: \\[E \\qty [ X] = \\frac{1 - p}{p}, \\quad Var \\qty [ X ] = \\frac{1 - p}{p^2}\\]\n\n\n\n\n幾何分布は無記憶性を持つ \\[\n    P(X = x + x_0 | X \\geq x_0) = P(X = x)\n\\tag{1}\\]\n\n証明 (無記憶性を持つことの証明). まず、\\(P(X \\geq x_0)\\) を計算する。無限級数の公式より \\[\nP(X \\geq x_0) = \\sum_{x = x_0}^{\\infty} p(1-p)^{x}\n= \\frac{p(1 - p)^{x_0}}{1 - (1 - p)} = (1 - p)^{x_0}\n\\] となる。 次に 式 1 を確かめる。 1\n\\[\n\\begin{aligned}\n    P(X = x + x_0 | X \\geq x_0) &= \\frac{P(X = x + x_0)}{P(X \\geq x_0)}  \\\\\n    &= \\frac{p(1-p)^{x + x_0}}{(1-p)^{x_0}} \\\\\n    &= p(1-p)^{x} = P(X = x)\n\\end{aligned}\n\\tag{2}\\]\n以上から幾何分布が無記憶性を持つことがわかった。\n\n\n証明 (非負整数上の確率分布が無記憶性を持つことの必要十分条件はその確率分布が幾何分布であることの証明). 十分性は上で示したので、必要性を示す。 式 1 が成立するとき、 \\[\nP(X = x + x_0) = P(X \\geq x_0) P(X = x)\n\\] となり、とくに \\(x_0 = 1\\) のとき \\[\n\\begin{aligned}\nP(X = x + 1) &= P(X \\geq 1) P(X = x) \\\\\n&= (1 - P(X = 0)) P(X = 0) \\\\\n&= (1 - p) P(X = 0)\n\\end{aligned}\n\\] という漸化式が得られる。ここで \\(P(X=0) = p\\) とおいた。 この漸化式より \\[\nP(X = x) = (1 - p) P(X = x - 1) = (1 - p)^{x} P(X = 0) = (1 - p)^{x}p\n\\] となり、幾何分布の確率関数になることがわかる。これで必要性が示された。\n\n\n\n\n\n\n\n意味:\nベルヌーイ試行の成功確率を \\(p\\) とし、 確率変数 \\(X\\) を \\(r\\) 回成功するまでの失敗の回数とする。 \\(X\\) は負の2項分布に従う。\n表記: \\(NB(r, p)\\)\nパラメータ\n\n成功回数: \\(r\\)\n成功確率: \\(p\\)\n\n確率関数: \\[\n\\begin{aligned}\np (k) &= \\binom{r + k - 1}{k} p^r (1-p)^{k} \\\\\n&= {}_{r} H_{k} ~ p^r (1-p)^{k}\n\\end{aligned}\n\\]\n確率母関数: \\[\n\\begin{aligned}\nG (s) &= \\qty (\\frac{p}{1 - qs})^{r} \\\\\n&= \\qty ( 1 - (s - 1) \\frac{q}{p})^{-r}\n\\end{aligned}\n\\]\n期待値・分散: \\[\nE \\qty [ X] = \\frac{r(1 - p)}{p}, \\quad\nVar \\qty [ X ] = \\frac{r(1 - p)}{p^2}\n\\]\n再生性:\n\\(X_1 \\sim NB(r_1, p), X_2 \\sim NB(r_2, p)\\) のとき、 \\(Y = X_1 + X_2\\)は\\(NB(r_1 + r_2, p)\\)に従う。\n\n\n\n\n確率関数に重複組合せ \\({}_{r} H_{k}\\) が出てくる意味について説明する。\n\\(r\\) 回の成功と \\(x\\) 回の失敗の並び方の場合の数を考える。 最後は成功となることが決まっているので、それぞれの失敗は はじめ～1回目の成功の間、1回目の成功～2回目の成功の間、・・・、 \\(r-1\\) 回目の成功～ \\(r\\) 回目の成功の間 のどこかに位置することになる。この並び方の場合の数は重複組合せ \\({}_{r} H_{k}\\) となる。\n\n\n\n\n\ngraph LR\nA[はじめ] -- A1 --> B[1回目の成功]\nB -- A2 --> C[2回目の成功]\nC -- A3 --> D[\"...\"]\nD -- Ar-1 --> E[r-1回目の成功]\nE -- Ar --> F[r回目の成功]\n\n\n\n\n\n図 1: A1からArのどこに失敗が位置するかの場合の数は重複組み合わせになる\n\n\n\n\n\n\n\n\n\n\n表記: \\(HG(N, M, n)\\)\nパラメータ\n\nくじの総数: \\(N\\)\nあたりの数: \\(M\\)\nくじを引く回数: \\(n\\)\n\n確率関数: \\[\np (x) = \\frac{{}_M C_{x} \\times {}_{N-M} C_{n-x}}{{}_{N} C_{n}} , \\quad\n  \\max \\{0, n - (N - M)\\} \\leq x \\leq \\min \\{n, M\\}\n\\]\n確率母関数:\n初等的な関数では表せない\n期待値・分散: \\[\nE \\qty [X] = n \\frac{M}{N}, \\quad\n  Var \\qty [ X ] = n \\frac{M}{N} \\qty (1 - \\frac{M}{N}) \\times \\frac{N - n}{N - 1}\n\\]\n\n\n\n\n\\(X = x\\) となる確率を求める。 \\(N\\) 個のくじから \\(n\\) 個を引く場合の数は \\({}_{N} C_{n}\\) である。 このうち、あたりを \\(x\\) 個引く場合の数は、 \\(M\\) 個のあたりから \\(x\\) 個をひき、 \\(N - M\\) 個のはずれから \\(n - x\\) 個ひく場合の数を考えれば良いので \\({}_{M} C_{x} \\times {}_{N - M} C_{n - x}\\) となる。 よって、求める確率は \\[\nP(X = x) = p(x) = \\frac{{}_M C_{x} \\times {}_{N-M} C_{n-x}}{{}_{N} C_{n}}\n\\] となる。\\(x\\) の範囲は以下のように決められる。 \\({}_M C_x = {}_M C_{M-x}\\) であり、 \\(x \\geq 0, M-x \\geq\\) を満たす必要がある。 つまり \\[x \\geq 0, \\quad M \\geq x\\] という条件が得られる。 同様に、\\({}_{N - M} C_{n - x} = {}_{N - M} C_{N - M - n + x}\\) であり、 \\(n - x \\geq 0, N - M - n + x \\geq 0\\) を満たす必要がある。 つまり \\[\nx \\geq n - (N - M), \\quad m \\geq x\n\\] という条件が得られる。 以上の条件を合わせて \\[\n\\max \\{0, n - (N - M)\\} \\leq x \\leq \\min \\{n, M\\}\n\\] となる。\n\n\n\n\n\n\n確率関数 \\[\\begin{aligned}\n  p(x_1, x_2, \\dots, x_k) & = \\frac{n!}{x_1! x_2! \\dots x_k!} p_1^{x_1} p_2^{x_2} \\dots p_k^{x_k}   \\\\\n                          & \\qquad (p_1 + p_2 + \\dots + p_k = 1, \\quad x_1 + x_2 + \\dots + x_k = n)\n  \\end{aligned}\\]\n\n\n\n\n\n\n\n\n\n\n表記: \\(N(\\mu, \\sigma^2)\\)\nパラメータ\n\n平均: \\(\\mu\\)\n分散: \\(\\sigma^2\\)\n\n密度関数: \\[\nf (x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp \\qty ( - \\frac{(x - \\mu)^2}{2 \\sigma^2} )\n\\]\n積率母関数: \\[\n\\phi(\\theta) = \\exp \\qty ( \\mu \\theta + \\frac{\\sigma^2}{2} \\theta^2 )\n\\]\n期待値・分散: \\[\nE \\qty [X] = \\mu, \\quad Var \\qty [ X ] = \\sigma^2\n\\]\n再生性:\n\\(X_1 \\sim N(\\mu_1, \\sigma_1^2), X_2 \\sim N(\\mu_2, \\sigma_2^2)\\) のとき、 \\(Y = X_1 + X_2\\) は \\(N(\\mu_1 + \\mu_2, \\sigma_1^2 + \\sigma_2^2)\\) に従う。\n\n\n\n\n\\[\n\\begin{aligned}\n    \\phi(\\theta) & = E \\qty [e^{\\theta X}]                                                                                                                                                                                           \\\\\n    & = \\int_{- \\infty}^{\\infty} \\dd{x} e^{\\theta x} \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\qty ( - \\frac{(x - \\mu)^2}{2 \\sigma^2} )                                                                                     \\\\\n    & = \\int_{- \\infty}^{\\infty} \\dd{x}  \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\qty ( - \\frac{(x - \\mu)^2}{2 \\sigma^2} + \\theta x )                                                                                      \\\\\n    & = \\int_{- \\infty}^{\\infty} \\dd{x}  \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\qty ( -\\frac{1}{2 \\sigma^2} \\qty (x - (\\mu + \\sigma^2 \\theta))^2 - \\frac{-(\\mu + \\sigma^2 \\theta)^2 + \\mu^2}{2 \\sigma^2} ) & (平方完成)      \\\\\n    & = \\int_{- \\infty}^{\\infty} \\dd{x}  \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\qty ( -\\frac{1}{2 \\sigma^2} \\qty (x - (\\mu + \\sigma^2 \\theta))^2) \\exp \\qty ( \\mu \\theta + \\frac{\\sigma^2}{2} \\theta^2 )                 \\\\\n    & = \\exp \\qty ( \\mu \\theta + \\frac{\\sigma^2}{2} \\theta^2 )                                                                                                                                            & (ガウス積分を実行)\n\\end{aligned}\n\\]\n\n\n\n以下のことを示す\n\n\n\\(\\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_i\\) と \\(s^2 = \\frac{1}{n - 1} \\sum_{i=1}^{n} (X_i - \\bar{X})^2\\) が互いに独立である\n\\(\\frac{(n - 1) s^2}{\\sigma^2}\\) は自由度 \\(n-1\\) のカイ2乗分布に従う\n\n\n標準正規分布でない場合は標準化したものを考えればよいので、 一般性を失わず、 \\(X_1, \\dots, X_n \\sim N(0, 1), i.i.d.\\) とする。 ベクトルの形で \\(\\va{X} = (X_1, \\dots, X_n)^{T}\\) とすると、 \\(\\va{X} \\sim N_n(\\va{0}, I_n)\\) である。\n直交行列 \\(G\\) を考える。\\(G\\) の一行目はすべて \\(\\frac{1}{\\sqrt{n}}\\) であるとする。 他の行は互いに直行するように構成すれば良い。 \\(\\va{Y}\\) を \\(\\va{Y} = G\\va{X}\\) で定義する。 \\(G\\) は直交行列なので、\\(\\va{Y} \\sim N_n(\\va{0}, I_n)\\) となり、 \\(Y_1, \\dots, Y_n\\) は互いに独立となる。 このとき、 \\[\n\\sum_{i=1}^{n} Y_i^2 = \\va{Y}^{T} \\va{Y}\n  = \\va{X}^T G^{T} G \\va{X} = \\va{X}^{T} \\va{X} = \\sum_{i=1}^{n} X_i^2\n\\]\nまた、 \\[Y_1 = \\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} X_i = \\sqrt{n}\\bar{X}\\] となる。 偏差平方和を計算すると \\[\n\\sum_{i=1}^{n} (X_i - \\bar{X})^2\n  = \\sum_{i=1}^{n} X_i^2 - 2 \\bar{X} \\sum_{i=1}^{n} X_i + n \\bar{X}^2\n  = \\sum_{i=1}^{n} Y_i^2 - n \\bar{X}^2\n  = \\sum_{i=1}^{n} Y_i^2 - Y_1^2\n  = \\sum_{i=2}^{n} Y_i^2\n\\] となる。 これは互いに独立な標準正規分布に従う変数の二乗和であるから、 \\(\\sum_{i=2}^{n} Y_i^2 = \\frac{(n - 1) s^2}{\\sigma^2}\\) が自由度 \\(n-1\\) のカイ2乗分布に従うことがわかった。 また、 \\(s^2 = \\frac{1}{n - 1} \\sum_{i=1}^{n} (X_i - \\bar{X})^2\\) が \\(Y_2, \\dots, Y_n\\) のみに依存し、 \\(Y_1 = \\sqrt{n} \\bar{X}\\) に依存しないことから \\(s^2\\) と \\(\\bar{X}\\) が独立であることがわかった。\n\n\n\n\n\n\n表記: \\(N_n(\\va{\\mu}, \\Sigma)\\)\nパラメータ:\n\n平均ベクトル: \\(\\va{\\mu}\\)\n分散共分散行列: \\(\\Sigma\\)\n\n密度関数: \\[\nf (\\va{x})\n= \\frac{1}{\\sqrt{2\\pi}^n \\sqrt{|\\Sigma|}}\n\\exp \\qty ( - \\frac{1}{2} (\\va{x} - \\va{\\mu})^{T} \\Sigma^{-1} (\\va{x} - \\va{\\mu}) )\n\\]\n積率母関数: \\[\n\\phi(\\va{\\theta})\n= \\exp \\qty ( \\va{\\mu}^{T} \\va{\\theta} + \\frac{1}{2} \\va{\\theta}^{T} \\Sigma \\va{\\theta} )\n\\]\n期待値・分散: \\[\nE \\qty [\\va{X}] = \\va{\\mu}, \\quad Var \\qty [ \\va{X} ] = \\Sigma\n\\]\n再生性:\n\\(\\va{X}_1 \\sim N(\\va{\\mu}_1, \\Sigma_1), \\va{X}_2 \\sim N(\\va{\\mu}_2, \\Sigma_2)\\) のとき、 \\(\\va{Y} = \\va{X}_1 + \\va{X}_2\\)は\\(N(\\va{\\mu}_1 + \\va{\\mu}_2, \\Sigma_1 + \\Sigma_2)\\) に従う。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n定義:\n\\(Z_i \\sim N(0, 1), i = 1, \\dots, n\\) で、これらが互いに独立なとき \\(Y = Z_1^2 + \\dots + Z_n^2\\) が従う分布を自由度 \\(n\\) のカイ2乗分布という。\n表記: \\(\\chi^2(n)\\)\nパラメータ\n\n自由度: \\(n\\)\n\n密度関数: \\[\nf (x) = \\frac{1}{\\Gamma(\\frac{n}{2}) 2^{\\frac{n}{2}}}\n  y^{\\frac{n}{2} - 1} e^{- \\frac{y}{2}}, \\quad (y > 0)\n\\]\n積率母関数: \\[\\phi(\\theta) =\\]\n期待値・分散: \\[\nE \\qty [X] = n, \\quad Var \\qty [ X ] = 2n\n\\]\n再生性:\n\\(X_1 \\sim \\chi^2(n_1), X_2 \\sim \\chi^2(n_2)\\) のとき、 \\(Y = X_1 + X_2\\) は \\(\\chi^2(n_1 + n_2)\\) に従う。\n\n\n\n\n\n\n\n定義:\n\\(X_1 \\sim \\chi^2(n_1), X_2 \\sim \\chi^2(n_2)\\) でこれらが互いに独立なとき、 \\[\n  \\frac{X_1/n_1}{X_2/n_2}\n  \\]\nが従う分布を自由度 \\((n_1, n_2)\\) の \\(F\\) 分布という。\n表記: \\(F(n_1, n_2)\\)\nパラメータ:\n\n自由度: \\((n_1, n_2)\\)\n\n密度関数: \\[\nf (x) =\n\\]\n積率母関数: \\[\n\\phi(\\theta) =\n\\]\n期待値・分散: \\[\nE \\qty [X] = , \\quad Var \\qty [ X ] =\n\\]\n\n\n\n\n\n\n\\(Z \\sim N(0, 1), Y \\sim \\chi^2(n)\\) で、これらが互いに独立なとき、 \\[T = \\frac{Z}{\\sqrt{Y/n}}\\] が従う分布を自由度\\(n\\)の\\(t\\)分布という。\n\\[t(n)\\]\n\\(n\\)\n\\[f (x) = \\frac{\\Gamma(\\frac{n+1}{2})}{\\sqrt{\\pi n}\\Gamma{\\frac{n}{2}}}\n    \\qty ( 1 + \\frac{t^2}{n} )^{- \\frac{n+1}{2}}\\]\n\\[\\phi(\\theta) =\\]\n\\[E \\qty [X] = 0 \\quad (n > 1), \\quad Var \\qty [ X ] = \\frac{n}{n - 2} \\quad (n > 2)\\]\n\n\n\n\n\n\\[Ex(\\lambda)\\]\n\\(\\lambda > 0\\)\n\\[f(x) = \\lambda e^{- \\lambda x}, \\quad x > 0\\]\n\\[\\phi (\\theta) = \\frac{\\lambda}{\\lambda - \\theta}\\]\n\\[E \\qty [ X] = \\frac{1}{\\lambda}, \\quad Var \\qty [ X ] = \\frac{1}{\\lambda^2}\\]\n\n\n\n指数分布は無記憶性を持つ \\[\\label{eq:exponential_distribution_memorylessness}\n  P(T = t + t_0 | T \\geq t_0) = P(T = t)\\]\n\n\n\\(T \\sim Ex(\\lambda)\\)とする。 \\(T\\)の確率密度関数は\\(f(t) = \\lambda e^{-\\lambda t}\\)である。 \\(T > t_0\\)が与えられたときの\\(T\\)の条件付き確率密度関数を求める。 \\[\\begin{aligned}\n    f_{T | T > t_0} (t) \\dd{t} &= P( t \\leq T \\leq t + \\dd{t} | T > t_0) \\\\\n    &= \\frac{P( t \\leq T \\leq t + \\dd{t} , T > t_0)}{P(T > t_0)}  \\\\\n    &= \\frac{P( t \\leq T \\leq t + \\dd{t} , t > t_0)}{\\int_{t_0}^{\\infty}\\dd{t}f(t)} \\\\\n    &= \\frac{f(t)\\dd{t} \\times I_{[t>t_0]}(t)}{e^{-\\lambda t_0}} \\\\\n    &= \\lambda e^{-\\lambda (t - t_0)} I_{[t>t_0]}(t) \\dd{t}\n  \\end{aligned}\\] であるから、 \\[f_{T | T > t_0} (t) =  \\lambda e^{-\\lambda (t - t_0)}  \\quad (t > t_0)\\] となる。 これは基準点を\\(t_0\\)にずらした変数\\(\\tilde{T} = T - t_0\\)の確率密度関数と等しくなる。 指数分布が無記憶性を持つことがわかった。\n\n\n\n十分性は上で示したので、必要性を示す。 [eq:exponential_distribution_memorylessness]が成り立つとき、 \\[f(t + t_0) = (1 - F(t_0)) f(t) = f(t) - F(t_0) f(t)\\] となる。ここで\\(t_0 = \\Delta t\\)と書き、\\(\\Delta t\\)が小さい時を考える。 すると \\[\\frac{f(t + \\Delta t) - f(t)}{\\Delta t} =\n   - \\frac{1}{\\Delta t} \\qty ( \\int_{0}^{\\Delta t} \\dd{t} f(t)) f(t)\\] と書くことができ、\\(\\Delta t \\rightarrow 0\\)で上式は \\[f'(t) = - f(0) f(t)\\] という微分方程式が得られる。\\(f(0)\\)は定数であるので、\\(f(0)=\\lambda\\)とおく。 この微分方程式を解くと \\[f(t) = \\lambda e^{- \\lambda t}\\] となる。ここで全体にかかる係数\\(\\lambda\\)は上で決めた\\(f(0)=\\lambda\\)から定まる。 これは指数分布の密度関数である。これで必要性が示された。\n\n\n\n\n\n\n\n\nガンマ分布の密度関数\n\n\n\n\\[Ga(\\nu, \\alpha)\\]\n\\(\\nu > 0\\) \\(\\alpha > 0\\)\n\\[f(x) = \\frac{1}{\\alpha^{\\nu} \\Gamma (\\nu)} x ^ {\\nu - 1} e ^ {- \\frac{x}{\\alpha}}, \\quad x >0\\]\n\\[\\phi (\\theta) = ( 1 - \\theta \\alpha) ^ {- \\nu}\\]\n\\[E \\qty [ X] = \\nu \\alpha, \\quad Var \\qty [ X ] = \\nu \\alpha ^2\\]\n\\(X_1 \\sim Ga(\\nu_1, \\alpha), X_2 \\sim Ga(\\nu_2, \\alpha)\\) のとき、 \\(Y = X_1 + X_2\\)は\\(Ga(\\nu_1 + \\nu_2, \\alpha)\\)に従う。\n\n\n\n\\(X \\sim Ga(\\nu, 1)\\)とする。\n\\(X\\)の期待値 \\[\\begin{aligned}\n    E \\qty [X] & = \\int_0^{\\infty} \\frac{x}{\\Gamma(\\nu)} x^{\\nu - 1} e^{-x} \\dd{x}\n    = \\frac{1}{\\Gamma(\\nu)} \\int_0^{\\infty}  x^{(\\nu + 1) - 1} e^{-x} \\dd{x}       \\\\\n               & = \\frac{\\Gamma(\\nu + 1)}{\\Gamma(\\nu)}\n    = \\frac{\\nu !}{(\\nu - 1)!}                                                     \\\\\n               & = \\nu\n  \\end{aligned}\\]\n\\(X^2\\)の期待値 \\[\\begin{aligned}\n    E \\qty [X^2] & = \\int_0^{\\infty} \\frac{x^2}{\\Gamma(\\nu)} x^{\\nu - 1} e^{-x} \\dd{x}\n    = \\frac{1}{\\Gamma(\\nu)} \\int_0^{\\infty}  x^{(\\nu + 2) - 1} e^{-x} \\dd{x}           \\\\\n                 & = \\frac{\\Gamma(\\nu + 2)}{\\Gamma(\\nu)}\n    = \\frac{(\\nu + 1)!}{(\\nu - 1)!}                                                    \\\\\n                 & = (\\nu + 1)\\nu\n  \\end{aligned}\\]\n\\(X\\)の分散 \\[Var \\qty [X] = E \\qty [X^2] - (E \\qty [X])^2 = (\\nu + 1)\\nu - \\nu^2 = \\nu\\]\n\\(Y = \\alpha X\\)とすると、\\(Y \\sim Ga(\\nu, \\alpha)\\)となる。\n\\(Y\\)の期待値 \\[E \\qty [Y] = E \\qty [\\alpha X] = \\alpha \\nu\\]\n\\(Y\\)の分散 \\[Var \\qty [Y] = Var \\qty [\\alpha X] = \\alpha^2 \\nu\\]\n\n\n\n\n\\(X \\sim Be(\\alpha, \\beta)\\)のとき、\\(1 - X \\sim Be(\\beta, \\alpha)\\)"
  },
  {
    "objectID": "03-statistics/mathematical-statistics/index.html#分布の間の関係",
    "href": "03-statistics/mathematical-statistics/index.html#分布の間の関係",
    "title": "数理統計学",
    "section": "2 分布の間の関係",
    "text": "2 分布の間の関係\n\n2.1 ガンマ分布とカイ2乗分布\n\\(Ga(\\frac{n}{2}, 2)\\)は自由度\\(n\\)のカイ2乗分布になる\n\n\n2.2 ガンマ分布と指数分布\n\\(Ga(1, \\frac{1}{\\lambda})\\)はパラメータ\\(\\lambda\\)の指数分布\\(Ex(\\lambda)\\)になる\n\n\n2.3 ガンマ分布と負の二項分布\nガンマ分布は負の2項分布を連続変数化したものと考えられる\n\nポアソン分布に従うイベントが\\(\\nu\\)回起こるまでの時間。 ポアソン分布に従うイベントが1回起こるまでの時間。\nベルヌーイ試行が\\(r\\)回成功するまでの失敗の回数。 ベルヌーイ試行が1回成功するまでの失敗の回数。\n\n\n\n2.4 二項分布とポアソン分布\n\\[\\begin{aligned}\n    p(x) &= \\binom{n}{x} p^{x} (1 - p)^{n -x} \\\\\n    &= \\frac{n!}{x! (n - x)!} \\qty (\\frac{\\lambda}{n})^{x} \\qty (1 - \\frac{\\lambda}{n})^{n - x}  & (\\lambda = np) \\\\\n    &= \\frac{1}{x!} n \\cdot (n - 1) \\cdot \\dots \\cdot (n - x + 1) \\qty (\\frac{\\lambda}{n})^{x} \\qty (1 - \\frac{\\lambda}{n})^{n - x} \\\\\n    &= \\frac{1}{x!} 1 \\cdot \\qty(1 - \\frac{1}{n}) \\cdot \\dots \\cdot \\qty (1 - \\frac{x - 1}{n})\n    \\lambda^{x} \\qty (1 - \\frac{\\lambda}{n})^{n}  \\qty (1 - \\frac{\\lambda}{n})^{-x} \\\\\n    &= \\frac{1}{x!} 1 \\cdot \\qty(1 - \\frac{1}{n}) \\cdot \\dots \\cdot \\qty (1 - \\frac{x - 1}{n})\n    \\lambda^{x} \\qty { \\qty (1 - \\frac{\\lambda}{n})^{-\\frac{n}{\\lambda}} }^{- \\lambda} \\qty (1 - \\frac{\\lambda}{n})^{-x} \\\\\n    &\\rightarrow \\frac{\\lambda^x}{x!}e^{-\\lambda} & (n \\rightarrow \\infty)\n  \\end{aligned}\\]\n\n\n2.5 指数分布とポアソン分布\n\n2.5.1 ポアソン分布から指数分布を導く\n単位時間あたり平均で\\(\\lambda\\)回ランダムに発生するイベントが 時間\\((0, t)\\)の間に発生する回数は\\(Po(\\lambda t)\\)に従う。 \\(X \\sim Po(\\lambda t)\\)とする。\\(X\\)の確率関数は \\[p(x) = \\frac{(\\lambda t)^x}{x!}e^{-\\lambda t}\\] である。 イベントが1回発生するまでの時間を表す確率変数を\\(T\\)とする。 \\(T\\)の分布関数は \\[\\begin{aligned}\n    F(t) &= P(T \\leq t) \\\\\n    &= 1 - P(T > t) \\\\\n    &= 1 - (時間(0, t)内に一回もイベントが発生しない確率) \\\\\n    &= 1 - P(X = 0) \\\\\n    &= 1 - p(x=0) \\\\\n    &= 1 - e^{-\\lambda t}\n  \\end{aligned}\\] となり、これは指数分布の分布関数である。 密度関数を計算すると \\[f(t) = \\dv{t} F(t) = \\lambda e^{- \\lambda t}\\] となる。\n\n\n2.5.2 指数分布からポアソン分布を導く\n\\(T_i\\)をイベントが発生するまでの時間を表す確率変数とし、\\(T_i \\sim Ex(\\lambda)\\)とする。 \\[U_k = \\sum_{i=1}^{k} T_i\\] とし、\\(X\\)を \\[X = (U_k < t を満たす最大のk)\\] と定義する。つまり\\(X\\)は時刻\\(t\\)までに発生したイベントの回数を表す確率変数である。 \\(X=k\\)となる確率を求めると \\[\\begin{aligned}\n    P(X = k) &= (k回目のイベントが時刻(0, t)の間に発生し、k+1回目のイベントは時刻tより後に発生する確率) \\\\\n    &= \\int_{0}^{t} P(U_k = s, U_{k+1} > t) \\dd{s}\n    = \\int_{0}^{t} P(U_k = s) P(U_{k+1} > t | U_k = s) \\dd{s} \\\\\n    &= \\int_{0}^{t} P(U_k = s) P(T_{k+1} > t-s) \\dd{s}\n    = \\int_{0}^{t} \\frac{\\lambda^k}{\\Gamma(k)} s^{k-1} e^{-\\lambda t}\n    e^{- \\lambda (t-s)} \\dd{s}  \\\\\n    &= \\frac{\\lambda^k}{(k -1)!} e^{-\\lambda t} \\int_{0}^{t} s^{k-1} \\dd{s}\n    = \\frac{\\lambda^k}{(k -1)!}e^{-\\lambda t} \\frac{1}{k} t^{k} \\\\\n    &= \\frac{(\\lambda t)^k}{k!}e^{-\\lambda t}\n  \\end{aligned}\\] となり、これは\\(Po(\\lambda t)\\)の確率関数である。 途中で\\(U_k \\sim Ga(k, 1/\\lambda)\\)であることを使った。\n\n\n\n2.6 指数分布と幾何分布\n\\(T\\)をイベントが発生するまでの時間を表す確率変数とし、\\(T \\sim Ex(\\lambda)\\)とする。 時間を間隔\\(d\\)で離散化する。 離散化した各時間の中で初めてイベントが発生する確率は \\[\\begin{aligned}\n  &時間(0, d]の間に初めてイベントが発生する確率 &&= P(T \\leq d) = 1 - e^{- \\lambda d} \\\\\n  &時間(d, 2d]の間に初めてイベントが発生する確率 = P(T \\leq 2d | T > d) &&= P(T \\leq d) = 1 - e^{- \\lambda d} \\\\\n  &時間(2d, 3d]の間に初めてイベントが発生する確率 = P(T \\leq 3d | T > 2d) &&= P(T \\leq d) = 1 - e^{- \\lambda d} \\\\\n  & &&\\vdots\n\\end{aligned}\\] となる。ここで指数分布の無記憶性を使った。 つまり各時間間隔\\(d\\)のなかで初めてイベントが発生する確率は常に等しく \\(p = 1 - e^{-\\lambda d}\\)となる。\n\\(x\\)を整数とする。時刻\\(xd\\)までイベントが発生せず、 時間\\((xd, (x+1)d]\\)の間に初めてイベントが起こったとき\\(X=x\\)となるように確率変数\\(X\\)を定める。 このとき、\\(X\\)はパラメータ\\(p = 1 - e^{-\\lambda d}\\)の幾何分布\\(Ge(p)\\)に従う。"
  },
  {
    "objectID": "03-statistics/mathematical-statistics/index.html#十分統計量",
    "href": "03-statistics/mathematical-statistics/index.html#十分統計量",
    "title": "数理統計学",
    "section": "3 十分統計量",
    "text": "3 十分統計量\n\n3.1 概要\n\n3.1.1 十分統計量の定義\n\\(k\\)個の統計量\\(T = ( T_1, \\dots, T_k )\\)がパラメータ\\(\\theta\\)に関する\\(k\\)次元の十分統計量であるとは、 \\(T\\)を与えられたとき\\(X = ( X_1, \\dots, X_n )\\)の条件付き分布が\\(\\theta\\)に依存しないことである。 \\[\\label{eq:sufficient_statistic_definition}\n  P(X | T, \\theta) = P(X | T)\\]\nベイズ統計のように\\(\\theta\\)を確率変数として考えれば、[eq:sufficient_statistic_definition]は、 \\(T\\)を与えたときに\\(X\\)と\\(\\theta\\)が条件付き独立となることを意味する。 \\(X\\)と\\(\\theta\\)が条件付き独立ということは、\\(T\\)さえ知っていればそれ以上\\(X\\)から\\(\\theta\\)の情報が得られないということ。 つまり\\(T\\)だけ知っていれば\\(X\\)から得られる\\(\\theta\\)に関する情報はすべてわかるということ。\n\n\n3.1.2 分解定理\n\\(X\\)を離散確率変数または連続確率変数とし\\(p_\\theta\\)を\\(X\\)の確率関数または密度関数とする。 \\(T(X) = ( T_1(X), \\dots, T_k(X))\\)が十分統計量であるための必要十分条件は\\(p_\\theta\\)が \\[p_\\theta (x) = g_\\theta (T(x)) h(x)\\]\nの形に分解できることである。ここで\\(h(x)\\)は\\(\\theta\\)を含まない\\(x\\)のみの関数である。\n\n\n3.1.3 ラオ・ブラックウェルの定理\n\\(\\delta(X)\\)を未知パラメータ\\(\\theta\\)の推定量とし、 平均二乗誤差\\(R(\\theta, \\delta) = E_{\\theta} \\qty [(\\delta(X) - \\theta)^2]\\)をリスク関数とする。 ここで、十分統計量\\(T\\)を与えたときの\\(X\\)の条件付き分布を用いて\\(\\delta(X)\\)の条件付き期待値をとったものを\\(\\delta^{*}(T)\\)とする。 すなわち \\[\\delta^{*}(T) = E \\qty [ \\delta(X) | T ]\\] である。このとき次のことが成り立つ。 \\[E_{\\theta} \\qty [(\\delta^{*}(T) - \\theta)^2] \\leq E_{\\theta} \\qty [(\\delta(X) - \\theta)^2], \\quad \\forall \\theta\\] であり、等号が成立するのは\\(P_{\\theta}(\\delta(X) = \\delta^{*}(T)) = 1\\)となるときのみである。 2\n\n\n3.1.4 完備性の定義\n統計量\\(T(X)\\)が完備であるとは、\\(T\\)の関数\\(g(T)\\)のなかで恒等的にその期待値が0となるものは、 定数0に限るということである。 すなわち、任意の関数\\(g(T)\\)に対し \\[E_{\\theta} \\qty [g(T)] = 0, \\quad \\forall \\theta \\Rightarrow g(T) \\equiv 0\\] が成り立つならば\\(T\\)は完備である。 3\n\n\n\n3.2 ポアソン分布\n\n3.2.0.1 十分統計量の計算\n\\(X_1, \\dots, X_n \\sim Po(\\lambda), i.i.d.\\)とする。確率関数は \\[p_\\lambda(x) = \\prod_{i=1}^{n} \\frac{\\lambda^{x_i}}{x_i !} e^{- \\lambda}\n  = \\lambda ^{\\sum_{i=1}^{n} x_i} e^{-n \\lambda} \\qty ( \\prod_{i=1}^{n} x_i ! ) ^{-1}\\] と表される。ここで \\[g_\\lambda = \\lambda ^{\\sum_{i=1}^{n} x_i} e^{-n \\lambda}, \\quad h(x) = \\qty ( \\prod_{i=1}^{n} x_i ! ) ^{-1}\\] とおけば、\\(T = \\sum_{i=1}^{n}X_i\\)が十分統計量であることがわかる。\n\n\n\n3.3 正規分布\n\n3.3.0.1 一次元の十分統計量の計算\n\\(X_1, \\dots, X_n \\sim N(\\mu, 1), i.i.d.\\)とする。\\(X\\)の同時密度関数は \\[\\begin{aligned}\n    f(x) & = \\frac{1}{(2\\pi)^{n/2}} \\exp \\qty [ - \\frac{1}{2} \\sum_{i=1}^{n} (x_i - \\mu)^2 ]                                                                                                      \\\\\n         & = \\frac{1}{(2\\pi)^{n/2}} \\exp \\qty [ - \\frac{1}{2} \\sum_{i=1}^{n} (x_i - \\bar{x} + \\bar{x} - \\mu)^2 ]                                                                                  \\\\\n         & = \\frac{1}{(2\\pi)^{n/2}} \\exp \\qty [ - \\frac{1}{2} \\sum_{i=1}^{n} (x_i - \\bar{x})^2  - \\sum_{i=1}^{n} (x_i - \\bar{x})(\\bar{x} - \\mu)  - \\frac{1}{2} \\sum_{i=1}^{n} (\\bar{x} - \\mu)^2 ] \\\\\n         & = \\frac{1}{(2\\pi)^{n/2}} \\exp \\qty [ - \\frac{1}{2} \\sum_{i=1}^{n} (x_i - \\bar{x})^2 - \\frac{n (\\bar{x} - \\mu)^2}{2} ]\n  \\end{aligned}\\] と表すことができる。したがって、 \\[g_\\mu = \\frac{1}{(2\\pi)^{n/2}} \\exp \\qty (- \\frac{n (\\bar{x} - \\mu)^2}{2}), \\quad\n  h(x) = \\exp \\qty (- \\frac{1}{2} \\sum_{i=1}^{n} (x_i - \\bar{x})^2)\\] とおけば、分解定理より\\(\\bar{X}\\)が十分統計量であることがわかる。\n\n\n\n3.4 一様分布\n\n3.4.0.1 一次元の十分統計量の計算\n\\(X_1, ..., X_n \\sim U[0, \\theta]\\)とする。同時密度関数は \\[\\label{eq:uniform_pdf}\n  f_\\theta (x) =\n  \\begin{dcases}\n    \\frac{1}{\\theta^{n}}, & \\text{ if } 0 \\leq x_i \\leq \\theta, \\forall i \\\\\n    0,                    & \\text{otherwise}\n  \\end{dcases}\\] と表される。ここで、\\(X_{max} = \\max_{i} X_i\\)とし、以下の関数を定義する。 \\[I _{[x_{max} \\leq \\theta]}(x) =\n  \\begin{dcases}\n    1, & \\text{ if } x_{max} \\leq \\theta \\\\\n    0, & \\text{ otherwise}\n  \\end{dcases}\\]\n\\[h(x) = h(x_1, \\dots, x_n) =\n  \\begin{dcases}\n    1, & \\text{ if } x_i \\geq 0, \\forall i \\\\\n    0, & \\text{ otherwise }\n  \\end{dcases}\\]\n\\(x_i \\geq 0, \\forall i \\Leftrightarrow x_{max} \\leq \\theta\\)であることに注意すれば、[eq:uniform_pdf]は \\[f_{\\theta} (x) = \\frac{1}{\\theta^n} I _{[x_{max} \\leq \\theta]}(x) h(x)\\] と書ける。したがって、 \\[g_{\\theta} = \\frac{1}{\\theta^n} I _{[x_{max} \\leq \\theta]}(x)\\] とおけば、分解定理より\\(T(X) = X_{max}\\)が十分統計量であることがわかる。"
  },
  {
    "objectID": "03-statistics/mathematical-statistics/index.html#不偏推定量",
    "href": "03-statistics/mathematical-statistics/index.html#不偏推定量",
    "title": "数理統計学",
    "section": "4 不偏推定量",
    "text": "4 不偏推定量\n\n4.1 概要\n\n4.1.1 不偏推定量の定義\n\\(\\hat{\\theta}\\)が不偏推定量であるとは、 \\[E_{\\theta} \\qty [\\hat(\\theta)(X)] = \\theta, \\quad \\forall \\theta\\] が成り立つことである。\n\n\n4.1.2 一様最小分散不偏推定量(Uniformly Minimum Variance Unbiased estimator, UMVU)の定義\n\\(\\hat{\\theta}\\)が不偏推定量であれば平均二乗誤差は分散に一致する。 \\[E_{\\theta} \\qty [( \\hat{\\theta} - \\theta )] = E_{\\theta} \\qty [( \\hat{\\theta} - E_{\\theta}[\\hat{\\theta}] )] = Var_{\\theta} \\qty [\\hat{\\theta}]\\] したがって、不偏推定量に限れば、分散を最小にする推定量が望ましい推定量となる。\n不偏推定量\\(\\hat{\\theta}^{*}\\)がUMVUであるとは、任意の不偏推定量\\(\\hat{\\theta}\\)について \\[Var_{\\theta} \\qty [\\hat{\\theta}^{*}] \\leq Var_{\\theta} \\qty [\\hat{\\theta}], \\quad \\forall \\theta\\] が成り立つことである。\n\n与えられた不偏推定量がUMVUであることを示す方法 フィッシャー情報量に基づくクラメル・ラオの不等式を用いる方法 完備十分統計量の理論を用いる方法\n\n\n\n4.1.3 フィッシャー情報量\n\\(X = (X_1, \\dots, X_n)\\)の同時密度関数あるいは同時確率関数を\\(f(x, \\theta)\\)で表す。 ここで、\\(\\theta\\)は1次元のパラメータとする。 このとき\\(\\theta\\)に関するフィッシャー情報量\\(I_n(\\theta)\\)は次式で定義される。 \\[\\begin{aligned}\n    I_n(\\theta) & = E_{\\theta} \\qty [\\qty (\\pdv{\\log f(x, \\theta)}{\\theta})^2]                        \\\\\n                & = \\int \\qty (\\frac{\\pdv{\\theta} f(x, \\theta)}{f(x, \\theta)}) ^2 f(x, \\theta) \\dd{x} \\\\\n                & = \\int \\frac{\\qty (\\pdv{\\theta} f(x, \\theta))^2}{f(x, \\theta)} \\dd{x}\n  \\end{aligned}\\]\n\\(X_1, \\dots, X_n\\)が独立同一分布に従うときには \\[I_n(\\theta) = nI_1(\\theta)\\] となる。\n\n\n4.1.4 クラメル・ラオの不等式\n不偏推定量の分散とフィッシャー情報量の間には次の不等式が成立する。 \\(\\hat{\\theta}\\)を\\(\\theta\\)の不偏推定量とする。このとき \\[Var_{\\theta} \\qty [\\hat{\\theta}] \\geq \\frac{1}{I_n(\\theta)}\\]\n\n\n4.1.5 不偏推定量がUMVUであることを示す\n\n4.1.5.1 フィッシャー情報量に基づくクラメル・ラオの不等式を用いる方法\n不偏推定量\\(\\hat{\\theta}^{*}\\)が \\[Var_{\\theta} \\qty [\\hat{\\theta}^{*}] = \\frac{1}{I_n(\\theta)}, \\quad \\forall \\theta\\] を満たせば、\\(\\hat{\\theta}^{*}\\)はUMVUである。\n\n\n4.1.5.2 完備十分統計量の理論を用いる方法\n\\(T\\)を完備十分統計量とする。このとき\\(T\\)の関数である不偏推定量\\(\\delta^{*}(T)\\)は一意的に定まり \\(\\delta^{*}(T)\\)はUMVUとなる。 また、任意の不偏推定量を\\(\\hat{\\theta}\\)とするとき \\(E[\\hat{\\theta} | T]\\)は\\(\\delta^{*}(T)\\)に一致する。\n以上のことは次のように確かめられる。 \\(\\hat{\\theta}\\)を不偏推定量とし、\\(T(X)\\)を（一般の）十分統計量とする。 \\[\\hat{\\theta}^{*}(t) = E \\qty [\\hat{\\theta}(X) | T(X) = t]\\] とおくと、期待値の繰り返しの公式により、 \\[\\theta = E_{\\theta} \\qty [\\hat{\\theta}(X)] = E_{\\theta}^{T} \\qty [E [\\hat{\\theta}|T]] = E_{\\theta}^{T} \\qty [\\hat{\\theta}^{*}(T)]\\] となるから、\\(\\hat{\\theta}^{*}(T)\\)も不偏推定量であることがわかる。 そしてラオ・ブラックウェルの定理により、\\(\\hat{\\theta}^{*}(T)\\)と\\(\\hat{\\theta}\\)が一致しない限り \\(Var_{\\theta} \\qty [\\hat{\\theta}^{*}(T)] < Var_{\\theta} \\qty [\\hat{\\theta}]\\) となる。 4\n次に、\\(T\\)を完備十分統計量とする。同様に \\[\\hat{\\theta}^{*}(t) = E \\qty [\\hat{\\theta}(X) | T(X) = t]\\] とおく。 ここで、\\(\\tilde{\\theta}\\)をほかの任意の不偏推定量とする。 \\(\\tilde{\\theta}\\)にラオ・ブラックウェルの定理を応用して \\[\\tilde{\\theta}^{*}(t) = E \\qty [\\tilde{\\theta}(X) | T(X) = t]\\] とおく。 このとき、\\(Var_{\\theta} [\\tilde{\\theta}] \\geq Var_{\\theta} [\\tilde{\\theta}^{*}]\\)である。 ところで、\\(g(T) = \\hat{\\theta}^{*}(T) - \\tilde{\\theta}^{*}(T)\\)とおくと、 \\[E_{\\theta} \\qty [g(T)] = E_{\\theta} \\qty [\\hat{\\theta}^{*}(T) - \\tilde{\\theta}^{*}(T)] = \\theta - \\theta = 0, \\quad \\forall \\theta\\] となる。したがって完備性の定義より\\(g(T) \\equiv 0\\)すなわち\\(\\hat{\\theta}^{*}(T) \\equiv \\tilde{\\theta}^{*}(T)\\)でなければならない。 このとき、\\(Var_{\\theta} \\qty [\\tilde{\\theta}^{*}] = Var_{\\theta} \\qty [\\hat{\\theta}^{*}]\\)となるから、 \\(Var_{\\theta} \\qty [\\tilde{\\theta}] \\geq Var_{\\theta} \\qty [\\hat{\\theta}^{*}]\\)となることが示された。 5"
  },
  {
    "objectID": "03-statistics/mathematical-statistics/index.html#最尤推定量",
    "href": "03-statistics/mathematical-statistics/index.html#最尤推定量",
    "title": "数理統計学",
    "section": "5 最尤推定量",
    "text": "5 最尤推定量\n\n5.1 多項分布\n対数尤度関数は \\[l(p_1, p_2, \\dots, p_k) = x_1 \\log p_1 + x_2 \\log p_2 + \\dots x_k \\log p_k + const.\\]\nこれを、\\(p_1 + p_2 + \\dots + p_k = 1\\)の条件の下で最大化する\\(p_i\\)を見つける。 Lagrangeの未定乗数\\(\\lambda\\)を用いて、関数 \\[\\tilde{l}(p_1, p_2, \\dots, p_k, \\lambda)\n  = x_1 \\log p_1 + x_2 \\log p_2 + \\dots x_k \\log p_k - \\lambda (p_1 + p_2 + \\dots + p_k - 1)\\] を定義する。これをパラメータで微分したものがゼロという条件から \\[\\pdv{\\tilde{l}}{p_i} = \\frac{x_i}{p_i} - \\lambda = 0\\] という式が得られる。これより \\[\\hat{p}_i = \\frac{x_i}{\\lambda}\\] が得られ、条件\\(p_1 + p_2 + \\dots + p_k = 1\\)から \\[\\lambda = n\\] がわかる。以上から、\\(p_i\\)の最尤推定量は \\[\\hat{p_i} = \\frac{x_i}{n}\\] となる。"
  },
  {
    "objectID": "03-statistics/mathematical-statistics/index.html#正規分布に関する検定",
    "href": "03-statistics/mathematical-statistics/index.html#正規分布に関する検定",
    "title": "数理統計学",
    "section": "6 正規分布に関する検定",
    "text": "6 正規分布に関する検定\n\n6.1 2標本の平均の検定（分散が未知の場合）\n分散\\(\\sigma_{A}^2\\)と\\(\\sigma_{B}^2\\)は未知とする。 各群の標本分散 \\[\\begin{aligned}\n    s_{A}^2 = \\frac{1}{n_{A} - 1} \\sum_{i = 1}^{n_A} (X_{Ai} - \\bar{X}_{A})^2 \\\\\n    s_{B}^2 = \\frac{1}{n_{B} - 1} \\sum_{i = 1}^{n_B} (X_{Bi} - \\bar{X}_{B})^2\n  \\end{aligned}\\] で\\(\\sigma_{A}^2\\)と\\(\\sigma_{B}^2\\)を推定する。 共通の母分散\\(\\sigma_{A}^2 = \\sigma_{B}^2 = \\sigma^2\\)を仮定すると、 2つの群をプールした標本分散 \\[s^2 = \\frac{(n_{A} - 1) s_{A}^2 + (n_{B} - 1) s_{B}^2}{n_{A} + n_{B} - 2}\\] \\(\\sigma^2\\)を推定する。 \\(\\frac{(n_{A} + n_{B} - 2) s^2}{\\sigma^2}\\)は自由度\\((n_{A} + n_{B} - 2)\\)のカイ2乗分布に従う。 6 共通の母分散を仮定すると\\(\\bar{X}_{A} - \\bar{X}_{B}\\)と\\(s^2\\)は互いに独立であるから、 帰無仮説\\(H_0:\\delta = \\delta_0\\)のもとで、検定統計量 \\[T = \\frac{ \\bar{X}_{A} - \\bar{X}_{B} - \\delta_{0}}{ s \\sqrt{ \\frac{1}{n_{A}} + \\frac{1}{n_{B}}}}\\] 自由度\\((n_{A} + n_{B} - 2)\\)の\\(t\\)分布に従う。\n\n\n6.2 1元配置分散分析\nモデル \\[y_{ij} \\sim N(\\mu(A_i),\\sigma^2)\\]\n一般平均 \\[\\mu = \\frac{1}{a} \\sum_{i = 1}^{a} \\mu (A_i)\\]\n因子の効果 \\[\\alpha_i = \\mu (A_i) - \\mu\\]\nこのとき、 \\[y_{ij} = \\mu + \\alpha_i + \\epsilon_{ij}, \\quad \\epsilon_{ij} \\sim N(0, \\sigma^2)\\] と書ける。\n帰無仮説　\\(H_0: \\alpha_1 = \\cdots = \\alpha_a = 0\\)\n対立仮説　\\(H_1:\\) =が少なくとも1つは成り立たない\nこの検定に分散分析を用いる\n総平方和 \\[{\n    S_T = \\sum_{i =1}^{a} \\sum_{j=1}^{n} (y_{ij}- \\bar{y})^2\n    = \\sum_{i =1}^{a} \\sum_{j=1}^{n} y_{ij}^{2} - an \\bar{y}^{2}, \\quad 自由度 \\phi_T = an -1\n  }\\]\n水準間平方和 \\[{\n    S_A = \\sum_{i =1}^{a} \\sum_{j=1}^{n} (\\bar{y}_{A_i}- \\bar{y})^2\n    = n \\sum_{i =1}^{a}  (\\bar{y}_{A_i}- \\bar{y})^2\n    = n \\sum_{i =1}^{a} \\bar{y}_{A_i}^{2} - an \\bar{y}^{2} , \\quad 自由度 \\phi_A = a -1\n  }\\]\n誤差平方和 7 \\[{\n    S_E = \\sum_{i =1}^{a} \\sum_{j=1}^{n} (y_{ij}- \\bar{y}_{A_i})^2 = S_{T} - S_{A} , \\quad 自由度 \\phi_E = a (n-1)\n  }\\]\n帰無仮説が成り立つとき、 \\[{\n    F = \\frac{V_A}{V_E} = \\frac{S_A / \\phi_A}{S_E / \\phi_E}\n  }\\] が自由度\\((\\phi_A,\\phi_E)\\)のF分布に従う。これから、F値を求め、\\(H_0\\)を検定する。\n帰無仮説のもとで、 \\[\\begin{aligned}\n  \\frac{\\bar{y}_{A_i} - \\mu(A_i)}{\\sigma / \\sqrt{n}} \\sim N(0, 1)                \\\\\n  \\frac{S_{E}}{\\sigma^2} = \\phi_{E} \\frac{V_{E}}{\\sigma^2} \\sim \\chi^2(\\phi_{E}) \\\\\n\\end{aligned}\\] が成り立ち、\\(\\bar{y}_{A_i}\\)と\\(V_{E}\\)は独立であるから \\[\\frac{\\frac{\\bar{y}_{A_i} - \\mu(A_i)}{\\sigma / \\sqrt{n}}}{\\sqrt{V_{E} / \\sigma^2}}\n  = \\frac{\\bar{y}_{A_i} - \\mu(A_i)}{\\sqrt{V_{E}/n}} \\sim t(\\phi_{E})\\] となる。以上から、 平均\\(\\mu (A_i)\\)の点推定 \\[{\n    \\hat{\\mu} (A_i) = \\bar{y}_{A_i}\n  }\\] 平均\\(\\mu (A_i)\\)の95%信頼区間 \\[{\n    \\bar{y}_{A_i} \\pm t_{0.025} (\\phi_E) \\sqrt{V_E / n}\n  }\\]"
  },
  {
    "objectID": "03-statistics/mathematical-statistics/index.html#デルタ法",
    "href": "03-statistics/mathematical-statistics/index.html#デルタ法",
    "title": "数理統計学",
    "section": "7 デルタ法",
    "text": "7 デルタ法\n確率変数\\(X\\)について、平均と分散が \\[E \\qty[X] = \\mu_{x}, \\quad V \\qty[X] = \\sigma_{x}^2\\] のようにわかっているとする。 このとき、\\(Y = g(X)\\)と変数変換を行った確率変数\\(Y\\)の平均や分散を近似的に求める。\n\n7.0.0.1 分散の近似\n\\(Y = g(X)\\)の1次までの展開は \\[Y = g(X) \\approx g(\\mu_{x}) + g'(\\mu_{x})  (X - \\mu_{x})\\] となり、この両辺の分散を取ることで \\[V[Y] \\approx \\qty (g'(\\mu_{x}))^2 V \\qty[X] = \\qty (g'(\\mu_{x}))^2 \\sigma_{x}^2\\] となる。\n\n\n7.0.0.2 平均の近似\n\\(Y = g(X)\\)の2次までの展開は \\[Y = g(X) \\approx g(\\mu_{x}) + g'(\\mu_{x})  (X - \\mu_{x})\n  + \\frac{1}{2} g^{''}(\\mu_{x}) (X - \\mu_{x})^2\\] となるから、この両辺の平均をとることで \\[E \\qty[Y] \\approx g(\\mu_{x}) + \\frac{1}{2} g^{''}(\\mu_{x}) V \\qty[X]\n  = g(\\mu_{x}) + \\frac{1}{2} g^{''}(\\mu_{x}) \\sigma_{x}^2\\] となる。"
  },
  {
    "objectID": "03-statistics/mathematical-statistics/index.html#指数分布族",
    "href": "03-statistics/mathematical-statistics/index.html#指数分布族",
    "title": "数理統計学",
    "section": "8 指数分布族",
    "text": "8 指数分布族\n(\\(k\\)母数)指数型分布族の密度関数 \\[f (x, \\theta) = h(x) \\exp \\qty ( \\sum_{j=1}^{k} T_j(x)\\psi_{j}(\\theta) - c(\\theta))\\]\n分解定理より、\\(T_1, \\cdots, T_k\\)が\\(k\\)次元の十分統計量をなす。"
  },
  {
    "objectID": "03-statistics/mathematical-statistics/index.html#生存時間解析",
    "href": "03-statistics/mathematical-statistics/index.html#生存時間解析",
    "title": "数理統計学",
    "section": "9 生存時間解析",
    "text": "9 生存時間解析"
  },
  {
    "objectID": "03-statistics/regression-analysis/heteroskedasticity.html",
    "href": "03-statistics/regression-analysis/heteroskedasticity.html",
    "title": "データ分析情報まとめ",
    "section": "",
    "text": "不均一分散\nAbadie ほか (2022)\nMacKinnon, Nielsen, と Webb (2022)\n\n\n\n\n\n参考文献\n\nAbadie, Alberto, Susan Athey, Guido W Imbens, と Jeffrey M Wooldridge. 2022. 「When Should You Adjust Standard Errors for Clustering?*」. The Quarterly Journal of Economics 138 (1): 1–35. https://doi.org/10.1093/qje/qjac038.\n\n\nMacKinnon, James G., Morten Ørregaard Nielsen, と Matthew D. Webb. 2022. 「Cluster-robust inference: A guide to empirical practice」. Journal of Econometrics. https://doi.org/10.1016/j.jeconom.2022.04.001."
  },
  {
    "objectID": "03-statistics/regression-analysis/index.html",
    "href": "03-statistics/regression-analysis/index.html",
    "title": "データ分析情報まとめ",
    "section": "",
    "text": "回帰分析"
  },
  {
    "objectID": "04-kaggle/index.html",
    "href": "04-kaggle/index.html",
    "title": "データ分析情報まとめ",
    "section": "",
    "text": "テンプレートのコードをまとめておいた方がよいもの\n\nベースラインモデルの作成\n特徴量重要度の算出\n特徴量の管理\nパラメータチューニング\n欠損値処理\n\n6.4. Imputation of missing values — scikit-learn 1.2.0 documentation\n\n欠損値処理を行うクラスがたくさんあるので見たほうがいい\n\n\nsklearn のパイプライン\n\nColumnTransformer\nFeatureUnion\n\n\n\n\n\n\nMLモデルでの予測が得意な部分と、ルールベースで決めた方が正確な部分とに分けるとよさそう。\n欠損値処理はどんな場合でもパイプラインに組み込む\n\n訓練データに欠損がなくても、推論データに欠損があるかも知れないから\n\n実行ファイルはpyファイルにするのがよさそう\n\nOSに依存しないから\n\n\n\n\n\nTarget Encoding はなぜ有効なのか - Speaker Deck\n\n「より水準が増えていくと」 のスライドを見るとわかりやすい\nラベルエンコードだと、カテゴリをターゲットの大きさの順を無視しているので効率が悪いということ\n\n\n\n\n\n考慮すべき観点\n\n\n\n\n\n実験を回しやすいか？\n再利用性が高いか？\n実運用に移すときに問題がないか？\n\n大幅にコードを書き直すということはしたくない\n\n\n\n参考リンク\n\n\n\n\n\narnabbiswas1/kaggle_pipeline_tps_aug_22: Kaggle Pipeline for tabular data competitions\nRobMulla/kaggle-ieee-fraud-detection: IEEE-CIS Fraud Detection Kaggle Competition Code\nHome - Cookiecutter Data Science\nオレオレKaggle実験管理\n機械学習実験環境を晒す - Qiita\nコンペ中のコード、どうしてる？ - Speaker Deck\nML Pipeline for Kaggleのススメ - 重み元帥によるねこにっき\nKazukiOnodera/Home-Credit-Default-Risk: 2nd Place Solution 💰🥈\n学習・推論パイプラインを構築する上で大切にしていること - Speaker Deck\nghmagazine/kagglebook\n\n「Kaggleで勝つデータ分析の技術」 のサンプルコード\n\ntakapy0210/takaggle: 分析コンペ用のスクリプト集"
  },
  {
    "objectID": "05-modeling/all-analytical-models/index.html",
    "href": "05-modeling/all-analytical-models/index.html",
    "title": "データ分析情報まとめ",
    "section": "",
    "text": "分析モデル入門\n本質を捉えたデータ分析のための分析モデル入門　 統計モデル、深層学習、強化学習等 用途・特徴から原理まで一気通貫！（杉山聡） | 書籍 本 | ソシム"
  },
  {
    "objectID": "05-modeling/all-analytical-models/part1.html",
    "href": "05-modeling/all-analytical-models/part1.html",
    "title": "データ分析情報まとめ",
    "section": "",
    "text": "理解思考\n\n\n\n\n\n目的\n\n対象の背後にある仕組みの理解\n\n分析で重視すること\n\n実験の環境を整えること\nうまく解釈できること\n\n結果得られるもの\n\n現実世界の背後に潜む法則\n\n\n\n応用思考\n\n\n\n\n\n目的\n\n知見を応用して、何かをうまくやる\n\n分析で重視すること\n\n現実世界に近いデータを用いること\n精度が高いこと\n\n結果得られるもの\n\nやりたいことをうまくやる方法、経験\n\n\n\n\n\n研究者の場合は、 結果の新規性・独創性、 手法の正当性・正確性が求められる。\nビジネス文脈のデータサイエンティストの場合、 ある程度の「ざっくり感」を持った素早い意志決定が重要になることもある。"
  },
  {
    "objectID": "05-modeling/index.html",
    "href": "05-modeling/index.html",
    "title": "データ分析情報まとめ",
    "section": "",
    "text": "モデリング"
  },
  {
    "objectID": "06-machine-learning/index.html",
    "href": "06-machine-learning/index.html",
    "title": "データ分析情報まとめ",
    "section": "",
    "text": "機械学習"
  },
  {
    "objectID": "06-machine-learning/LightGBM/index.html",
    "href": "06-machine-learning/LightGBM/index.html",
    "title": "データ分析情報まとめ",
    "section": "",
    "text": "LightGBM"
  },
  {
    "objectID": "06-machine-learning/sklearn/index.html",
    "href": "06-machine-learning/sklearn/index.html",
    "title": "データ分析情報まとめ",
    "section": "",
    "text": "scikit-learn"
  },
  {
    "objectID": "06-machine-learning/sklearn/utils.html",
    "href": "06-machine-learning/sklearn/utils.html",
    "title": "データ分析情報まとめ",
    "section": "",
    "text": "便利な関数\n\ncheck_cv\n_safe_indexing\nBunch"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "データ分析情報まとめ",
    "section": "",
    "text": "数理・データサイエンス・AI教育強化拠点コンソーシアム\n\n活用事例や解析手法の資料などがある\n\n各業界でのデータサイエンスの活用について調べてみた（随時追加）\nSimple Data Analytics Leads impact\n視聴率を予測したい\nR for Data Science\n\ntidyverseを用いたデータサイエンスの方法\n\nHands-On Predictive Analytics with Python [Book]\nMarketing-Mix-Modeling（MMM）に関する所感や問題意識について\n@bebebeBayes さんのTweet\nデータサイエンス・スクール　統計力向上サイト\n\n総務省統計局のデータサイエンスに関するサイト\nデータ分析コンペティションのページがあり、過去の受賞論文を見られる\n\nSTA 199\n【徹底解説】3変量正規分布の条件付き期待値と分散 | Academaid\nブレインパッドにおける機械学習プロジェクトの進め方\nPython Data Science Handbook | Python Data Science Handbook\nCS50 for Japanese: コンピュータサイエンスの入門 – 当ウェブサイトは、Creative Commons ライセンスに基づいて管理されています。\n[Clustering] How to sort a distance matrix\nMake Patterns Pop Out of Heatmaps with Seriation\nデータマイニング入門 Introduction to Data Mining | UTokyo OCWx\n【記事更新】私のブックマーク「不均衡データ分類」 – 人工知能学会 (The Japanese Society for Artificial Intelligence)\n【データ分析の必読10冊+差をつける10冊＋100冊超】データサイエンス、データ分析、機械学習関連の本 - Qiita\n数理・データサイエンス関連教材 | 東京大学 数理・情報教育研究センター\n\n\n\n\n意思決定論\n\n\n\n\n\nHow to Write a Data Science Project Report?\nDo you have any real example of Data Science reports?\n\nスタンフォード大学の講義のレポート？のようなものが紹介されている\n\nQuartoを使ってみませんか / quarto_get_started\n\n\n\n\n\nThe Python Graph Gallery\nR Graphics Cookbook, 2nd edition\nStanford CS course on data visualization techniques (Winter 2020)\nPandasのPlottingの全メソッドを解説 | 自調自考の旅\nChoosing color palettes — seaborn 0.12.1 documentation\n\n3種類の color palette の使い分けが説明されている\n\n\n\n\n\n\nNIST engineering statistics handbook\n\nEDAについて体系的にまとめられている\n\nExploratory data analysis using xgboost package in R\nAdvanced exploratory data analysis (EDA)\nExploratory Data Analysis\n\n\n\n\n\nFeature Engineering and Selection: A Practical Approach for Predictive Models\n\nEDAについても書かれている\n\n\n\n\n\n\nSQLが苦手という質問に対するTJOさんの解答\n\nOracle Master Goldを取ると良いらしい\n\n\n\n\n\n\n機械学習の文献に関するTwitterのスレッド\nFoundations of Machine Learning\nUnderstanding Machine Learning: From Theory to Algorithms\n\nDownloadからpdfをダウンロードできる\n\nChristoph MolnarさんはTwitterを使っています: 「SHAP, LIME, PFI, ... you can interpret ML models with many different methods. It’s all fun and games until two methods disagree. What if LIME says X1 has a positive contribution, SHAP says negative? A thread about the disagreement problem, and how to approach it:」 / Twitter\nSupervised Clustering: How to Use SHAP Values for Better Cluster Analysis\n大阪大学大学院講義 2022 on Vimeo\n\n機械学習のためのカーネルの講義動画\n\n一流の「ものさし」職人になろう　Cross Validation (交差検証)を深堀り - Qiita\n\n\n\n\n\nKaggle日記という戦い方\nKaggleの特徴量管理をPostgreSQLでやったら思ったより捗ったので,Dockerで誰でも使えるディレクトリを作ってみた - Qiita\nghmagazine/kagglebook\n\n「Kaggleで勝つデータ分析の技術」 のサンプルコード\n\n\n\n\n\n\n数理手法III Mathematical Method III | UTokyo OCWx\n\n東大が無料公開している数理最適化のコース\n\n\n\n\n\n\nlabml.ai Annotated PyTorch Paper Implementations\n\n\n\n\n\n[デモのプログラムあり] ガウス過程回帰(Gaussian Process Regression, GPR)におけるカーネル関数を11個の中から最適化する (scikit-learn) | データ化学工学研究室(金子研究室)＠明治大学 理工学部 応用化学科\nGaussian Processes for Machine Learning: Contents\nKernel Cookbook\n\n\n\n\n\n\n\n\nPython Asyncio: The Complete Guide\n【Python】よく使うモジュール: see | プログラマー＆天文博士＆占い師 ミサキのブログ\nBAyesian Model-Building Interface (Bambi) in Python — Bambi 0.9.3 documentation\n\nPythonでベイズモデリングを行うパッケージ\n\nHow to Access Sample Datasets in Pandas - Statology\n\npandas で適当なデータフレームを作る方法\n\n時系列データを前処理する際のPython逆引きメモ - EurekaMoments\n\n\n\n\n\nGetting Things in Order: An Introduction to the R Package seriation | Journal of Statistical Software\n\n\n\n\n\n\n仮説検定における再現性の問題と新たな方法論\n尤度比検定、ワルド検定、スコア検定をできるだけ分かりやすくまとめる【統計検定1級対策】\n時系列と情報量基準についてのTwitterの会話\nbiostatistics | バイオスタティスティクス | 生物統計学\n統計的工程管理の再考\n負の二項係数が重複組み合わせになることの簡単な理解 - 再発明した車輪でヤクの毛を刈りに行こう\n効果量と検定力分析入門 ―統計的検定を正しく使うために―\nThe Elements of Statistical Learning\n統計的仮説検定における効果量の概念と必要サンプルサイズの算出｜Dentsu Digital Tech Blog｜note\nオッズとは何か？オッズ比とリスク比の違い｜やわらか統計学－エビカツ横丁\nGitHub - genkuroki/Statistics: Notes of Statistics\n\nさまざまなノートが公開されている\n\n\n\n\n\n黒木玄 Gen Kuroki のTweet\nEMアルゴリズムとともだちになろう | ドクセル\n【徹底解説】EMアルゴリズムをはじめからていねいに | Academaid\nイェンゼンの不等式の３通りの証明 | 高校数学の美しい物語\n【徹底解説】3変量正規分布の条件付き期待値と分散 | Academaid\n偏相関係数の導出メモ - はしくれエンジニアもどきのメモ\n負の二項係数が重複組み合わせになることの簡単な理解 - 再発明した車輪でヤクの毛を刈りに行こう\nブレインパッドにおける機械学習プロジェクトの進め方\n確率変数の比の分布における平均と分散をデルタ法で求める - 人間だったら考えて\nデルタ法(Delta method) - 知識のサラダボウル\n\n\n\n\n\nRegression and Other Stories\n続・高橋セミナー: セミナー記録\n\nポアソン回帰に関することがいろいろ書かれている\n\n\n\n\n\n\nStatistical Modeling (2e)\n統計モデリング概論 DSHC 2021\nPythonで実装しながら緑本を学ぶ (第6章 GLMの応用範囲を広げる -ロジスティック回帰など-) - け日記\nベイズ統計モデリングと事前分布の話　前半パート.pdf\nベイズ統計モデリングと事前分布の話　後半パート.pdf\nAICの直感的な理解\n\nデータ解析のための統計モデリング入門 p.81に説明あり\n\n\n\n\n\n\n操作変数に関するTwitterのスレッド\nSensitivity Analyses for Unmeasured Confounders\n\n感度分析についての論文\n\n統計的因果推論、構造から見るか? 差分から見るか?：非巡回有向グラフ(DAG)、潜在反応モデル、そして構造的 (関数)因果モデルによる両者の統合的理解\n観察データを用いた因果推論で生じるバイアスの程度を考える：感度分析（Sensitivity analysis) & “E-value”入門\nE-valueを理解する：未計測の交絡因子によるバイアスを考える\n統計的因果推論は「交絡」の問題だけではない ～データサイエンティストが見落としがちな視点〜\n\n未測定交絡バイアスは有無ではなく大きさが重要と書かれている\n\n「因果関係」をとらえるために / To grasp causal relationship\nMoT TechTalk #14 タクシーアプリ『GO』の施策検証、因果推論が解決します\n統計的因果推論入門の講義資料を公開しました\n因果推論を学ぶにはどの教科書がいいですか\nCausal Inference: What If (the book) | Miguel Hernan’s Faculty Website | Harvard T.H. Chan School of Public Health\nSTAT 286/GOV 2003: Causal Inference\nWhich causal inference book you should read\n「学術的に理解する」ってどういうこと? ：統計的因果推論のフレームワークから見た 「量的×質的」に関する眺望の (いささかとっちらかった)スケッチを共有する - Speaker Deck\n統計的因果推論入門の講義資料を公開しました - Unboundedly\n因果推論の先へ―機械学習で因果効果を予測する『反実仮想機械学習（Counterfactual Machine Learning）』入門 | 株式会社ARISE analytics（アライズ アナリティクス）\n因果推論の道具箱\n\n\n\n\n\nBayes Rules! An Introduction to Applied Bayesian Modeling\n\nベイズモデリングの無料e-book\n\nベイズ深層学習(3.3~3.4) - Speaker Deck\nSpike and slab: Bayesian linear regression with variable selection - Batı Şengül\n\nベイズ線形回帰での正則化の方法\n\nAn Introduction to Bayesian Data Analysis for Cognitive Science\n社会科学におけるベイズ統計学\n[1307.5928] Understanding predictive information criteria for Bayesian models\nUpdating: A Set of Bayesian Notes\n(4) Statistical Rethinking 2022 - YouTube\n\n\n\n\n\n計量経済学 - 矢内 勇生\n\n\n\n\n\n\nコンサルファーム別、コンサルタントのプレゼン資料\n\n官公庁関連のプロジェクトに関する有名コンサルタントファームのプレゼン資料が紹介されている\n\n\n\n\n\nデータ分析職に採用されるために必要な「実務経験」をいかにして積むべきか\n\n\n\n\n\n\n\n\n論文の読み方 / How to survey\n\n論文調査の方法を紹介したスライド\n\n論文に何を書くべきか→これだけは埋めろ→論文作成穴埋めシート\n\n論文の書き方\n\n【論文の書き方がわかるおすすめ本5選】一気に書くための厳選本を紹介\n\n\n\n\n\n\nデータサイエンティスト(本社)の採用情報 | レバレジーズ株式会社"
  },
  {
    "objectID": "06-machine-learning/sklearn/ensemble.html",
    "href": "06-machine-learning/sklearn/ensemble.html",
    "title": "データ分析情報まとめ",
    "section": "",
    "text": "sklearn.ensemble\n\n\n\n学習時\n\nestimatorsはcross_val_predictでfold毎にモデルを学習して予測値を算出する\n推論時のために、全データでモデルを再学習し、estimators_に格納\n\n推論時\n\npredictを使うと、全データを使って再学習したモデルでの予測値が算出される\ntransformを使うと、final_estimator に入れる、各モデルの予測値が得られる"
  }
]